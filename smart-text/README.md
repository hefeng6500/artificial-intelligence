# æ™ºèƒ½è¾“å…¥æ³•é¡¹ç›® - æ–°æ‰‹å®Œæ•´æŒ‡å— ğŸš€

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½è¾“å…¥æ³•é¡¹ç›®ï¼Œèƒ½å¤Ÿæ ¹æ®ç”¨æˆ·è¾“å…¥çš„å‰å‡ ä¸ªå­—ç¬¦é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„å­—ç¬¦ã€‚

**æ ¸å¿ƒåŠŸèƒ½ï¼š** è¾“å…¥"è‡ªç„¶è¯­è¨€" â†’ é¢„æµ‹è¾“å‡º["å¤„ç†", "ç†è§£", "çš„", "æè¿°", "ç”Ÿæˆ"]

**æŠ€æœ¯æ ˆï¼š** Python + PyTorch + RNN

---

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
smart-text/
â”œâ”€â”€ src/                    # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ config.py          # é…ç½®æ–‡ä»¶ï¼ˆè¶…å‚æ•°è®¾ç½®ï¼‰
â”‚   â”œâ”€â”€ dataset.py         # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ model.py           # ç¥ç»ç½‘ç»œæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py        # æ¨¡å‹è¯„ä¼°
â”‚   â”œâ”€â”€ predict.py         # é¢„æµ‹è„šæœ¬
â”‚   â”œâ”€â”€ process.py         # æ•°æ®é¢„å¤„ç†
â”‚   â””â”€â”€ tokenizer.py       # åˆ†è¯å™¨
â”œâ”€â”€ data/                   # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/               # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/         # å¤„ç†åçš„æ•°æ®
â”œâ”€â”€ models/                 # ä¿å­˜çš„æ¨¡å‹
â”œâ”€â”€ logs/                   # è®­ç»ƒæ—¥å¿—
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜
```

---

## ğŸ¯ å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæŒ‰é¡ºåºæ‰§è¡Œï¼‰

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒé…ç½® âš™ï¸

é¦–å…ˆç¡®ä¿å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install torch pandas tqdm tensorboard
```

### ç¬¬äºŒæ­¥ï¼šç†è§£é…ç½®æ–‡ä»¶ ğŸ“

**æ–‡ä»¶ï¼š** `src/config.py`

è¿™ä¸ªæ–‡ä»¶å®šä¹‰äº†æ‰€æœ‰é‡è¦çš„å‚æ•°ï¼š

```python
# ========== æ¨¡å‹è¶…å‚æ•° ==========
SEQ_LEN = 5          # åºåˆ—é•¿åº¦ï¼šç”¨å‰5ä¸ªå­—ç¬¦é¢„æµ‹ç¬¬6ä¸ªå­—ç¬¦
BATCH_SIZE = 128     # æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡è®­ç»ƒå¤„ç†128ä¸ªæ ·æœ¬
EMBEDDING_DIM = 128  # è¯åµŒå…¥ç»´åº¦ï¼šå°†å­—ç¬¦è½¬æ¢ä¸º128ç»´å‘é‡
HIDDEN_SIZE = 256    # RNNéšè—å±‚å¤§å°ï¼š256ä¸ªç¥ç»å…ƒ

# ========== è®­ç»ƒè¶…å‚æ•° ==========
LEARNING_RATE = 1e-3 # å­¦ä¹ ç‡ï¼šæ§åˆ¶å‚æ•°æ›´æ–°é€Ÿåº¦
EPOCHS = 5           # è®­ç»ƒè½®æ•°ï¼šå®Œæ•´éå†æ•°æ®é›†5æ¬¡
```

**æ–°æ‰‹æç¤ºï¼š** è¿™äº›å‚æ•°å°±åƒåšèœçš„é…æ–¹ï¼Œå†³å®šäº†æ¨¡å‹çš„"å£å‘³"ï¼

### ç¬¬ä¸‰æ­¥ï¼šæ•°æ®å¤„ç† ğŸ“Š

**æ–‡ä»¶ï¼š** `src/dataset.py`

è¿™ä¸ªæ¨¡å—è´Ÿè´£å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ï¼š

```python
class InputMethodDataset(Dataset):
    def __init__(self, data_path):
        # è¯»å–JSONLæ ¼å¼çš„æ•°æ®
        # æ•°æ®æ ¼å¼ï¼š{"input":[2137,9117,5436,2747,7549], "target":7784}
        self.data = pd.read_json(data_path, lines=True, orient="records")

    def __getitem__(self, index):
        # å°†å­—ç¬¦IDè½¬æ¢ä¸ºPyTorchå¼ é‡
        input_tensor = torch.tensor(self.data[index]["input"], dtype=torch.long)
        target_tensor = torch.tensor(self.data[index]["target"], dtype=torch.long)
        return input_tensor, target_tensor
```

**å·¥ä½œåŸç†ï¼š**

1. åŸå§‹æ–‡æœ¬ï¼š"ä½ å¥½ä¸–ç•Œ"
2. è½¬æ¢ä¸º IDï¼š[1234, 5678, 9012, 3456]
3. åˆ›å»ºè®­ç»ƒæ ·æœ¬ï¼šè¾“å…¥[1234, 5678, 9012] â†’ ç›®æ ‡ 3456

### ç¬¬å››æ­¥ï¼šæ¨¡å‹æ¶æ„ ğŸ§ 

**æ–‡ä»¶ï¼š** `src/model.py`

è¿™æ˜¯é¡¹ç›®çš„"å¤§è„‘"ï¼Œä¸€ä¸ªä¸‰å±‚ç¥ç»ç½‘ç»œï¼š

```python
class InputMethodModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # ç¬¬1å±‚ï¼šè¯åµŒå…¥å±‚ - å°†å­—ç¬¦IDè½¬æ¢ä¸ºå‘é‡
        self.embedding = nn.Embedding(vocab_size, config.EMBEDDING_DIM)

        # ç¬¬2å±‚ï¼šRNNå±‚ - å­¦ä¹ åºåˆ—æ¨¡å¼
        self.rnn = nn.RNN(
            input_size=config.EMBEDDING_DIM,
            hidden_size=config.HIDDEN_SIZE,
            batch_first=True
        )

        # ç¬¬3å±‚ï¼šè¾“å‡ºå±‚ - é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦
        self.linear = nn.Linear(config.HIDDEN_SIZE, vocab_size)

    def forward(self, x):
        # æ•°æ®æµï¼šå­—ç¬¦ID â†’ å‘é‡ â†’ RNNå¤„ç† â†’ é¢„æµ‹æ¦‚ç‡
        embed = self.embedding(x)           # [batch, seq_len, embed_dim]
        output, hidden = self.rnn(embed)    # [batch, seq_len, hidden_dim]
        last_hidden = output[:, -1, :]      # [batch, hidden_dim]
        prediction = self.linear(last_hidden) # [batch, vocab_size]
        return prediction
```

**ç®€å•ç†è§£ï¼š**

- ğŸ”¤ **åµŒå…¥å±‚**ï¼šæŠŠ"ä½ "ã€"å¥½"è¿™äº›å­—è½¬æ¢æˆè®¡ç®—æœºèƒ½ç†è§£çš„æ•°å­—å‘é‡
- ğŸ”„ **RNN å±‚**ï¼šè®°ä½å‰é¢çš„å­—ï¼Œç†è§£ä¸Šä¸‹æ–‡å…³ç³»
- ğŸ¯ **è¾“å‡ºå±‚**ï¼šæ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„å­—

### ç¬¬äº”æ­¥ï¼šè®­ç»ƒè¿‡ç¨‹ ğŸ‹ï¸â€â™‚ï¸

**æ–‡ä»¶ï¼š** `src/train.py`

è¿™æ˜¯è®©æ¨¡å‹"å­¦ä¹ "çš„è¿‡ç¨‹ï¼š

```python
def train_one_epoch(model, dataloader, loss_function, optimizer, device):
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    epoch_total_loss = 0

    for inputs, targets in tqdm(dataloader, desc="è®­ç»ƒ"):
        # 1. æ•°æ®å‡†å¤‡
        inputs = inputs.to(device)   # è¾“å…¥åºåˆ—
        targets = targets.to(device) # æ­£ç¡®ç­”æ¡ˆ

        # 2. æ¸…é›¶æ¢¯åº¦
        optimizer.zero_grad()

        # 3. å‰å‘ä¼ æ’­ï¼šè®©æ¨¡å‹åšé¢„æµ‹
        outputs = model(inputs)

        # 4. è®¡ç®—æŸå¤±ï¼šé¢„æµ‹ä¸çœŸå®ç­”æ¡ˆçš„å·®è·
        loss = loss_function(outputs, targets)

        # 5. åå‘ä¼ æ’­ï¼šè®¡ç®—å¦‚ä½•æ”¹è¿›
        loss.backward()

        # 6. å‚æ•°æ›´æ–°ï¼šå®é™…æ”¹è¿›æ¨¡å‹
        optimizer.step()

        epoch_total_loss += loss.item()

    return epoch_total_loss / len(dataloader)
```

**è®­ç»ƒå°±åƒæ•™å°å­©è®¤å­—ï¼š**

1. ğŸ“– ç»™æ¨¡å‹çœ‹ä¾‹å­ï¼ˆå‰å‘ä¼ æ’­ï¼‰
2. âŒ å‘Šè¯‰å®ƒå“ªé‡Œé”™äº†ï¼ˆè®¡ç®—æŸå¤±ï¼‰
3. ğŸ”§ æ•™å®ƒæ€ä¹ˆæ”¹è¿›ï¼ˆåå‘ä¼ æ’­ï¼‰
4. âœ… è®©å®ƒè®°ä½æ”¹è¿›æ–¹æ³•ï¼ˆå‚æ•°æ›´æ–°ï¼‰

### ç¬¬å…­æ­¥ï¼šå®Œæ•´è®­ç»ƒæµç¨‹ ğŸ“

**ä¸»è®­ç»ƒå‡½æ•°ï¼š**

```python
def train():
    # 1. è®¾å¤‡é…ç½®ï¼šé€‰æ‹©GPUæˆ–CPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 2. æ•°æ®å‡†å¤‡ï¼šåŠ è½½è®­ç»ƒæ•°æ®
    dataloader = get_dataloader()

    # 3. æ¨¡å‹åˆå§‹åŒ–ï¼šåˆ›å»ºç¥ç»ç½‘ç»œ
    model = InputMethodModel(vocab_size=len(vocab_list)).to(device)

    # 4. è®­ç»ƒç»„ä»¶ï¼šæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_function = torch.nn.CrossEntropyLoss()  # å¤šåˆ†ç±»æŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)

    # 5. è®­ç»ƒå¾ªç¯ï¼šé‡å¤å­¦ä¹ å¤šè½®
    for epoch in range(1, config.EPOCHS + 1):
        print(f"========== ç¬¬ {epoch} è½®è®­ç»ƒ ==========")

        # è®­ç»ƒä¸€è½®
        avg_loss = train_one_epoch(model, dataloader, loss_function, optimizer, device)
        print(f"å¹³å‡æŸå¤±: {avg_loss}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            torch.save(model.state_dict(), "models/model.pt")
            print("âœ… æ¨¡å‹æ€§èƒ½æå‡ï¼Œå·²ä¿å­˜ï¼")
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### è¿è¡Œè®­ç»ƒ

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd smart-text

# å¼€å§‹è®­ç»ƒ
python src/train.py
```

### æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹

```bash
# å¯åŠ¨TensorBoardæŸ¥çœ‹è®­ç»ƒæ›²çº¿
tensorboard --logdir=logs
```

### ä½¿ç”¨æ¨¡å‹é¢„æµ‹

```bash
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
python src/predict.py
```

---

## ğŸ“ˆ è®­ç»ƒç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä½ ä¼šçœ‹åˆ°ï¼š

```
========== ç¬¬ 1 è½®è®­ç»ƒ ==========
è®­ç»ƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:45<00:00,  3.45it/s]
å¹³å‡æŸå¤±: 4.2341
âœ… æ¨¡å‹æ€§èƒ½æå‡ï¼Œå·²ä¿å­˜ï¼

========== ç¬¬ 2 è½®è®­ç»ƒ ==========
è®­ç»ƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 156/156 [00:43<00:00,  3.58it/s]
å¹³å‡æŸå¤±: 3.8765
âœ… æ¨¡å‹æ€§èƒ½æå‡ï¼Œå·²ä¿å­˜ï¼
```

**æŸå¤±å€¼è¶Šå° = æ¨¡å‹è¶Šèªæ˜ï¼**

---

## ğŸ¯ é¡¹ç›®äº®ç‚¹

- âœ¨ **æ–°æ‰‹å‹å¥½**ï¼šæ¯è¡Œä»£ç éƒ½æœ‰è¯¦ç»†ä¸­æ–‡æ³¨é‡Š
- ğŸ”§ **æ¨¡å—åŒ–è®¾è®¡**ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç†è§£å’Œä¿®æ”¹
- ğŸ“Š **å¯è§†åŒ–è®­ç»ƒ**ï¼šæ”¯æŒ TensorBoard å®æ—¶ç›‘æ§
- ğŸš€ **å¿«é€Ÿä¸Šæ‰‹**ï¼šä¸€é”®è¿è¡Œï¼Œç«‹å³ä½“éªŒ AI è®­ç»ƒè¿‡ç¨‹

---

## ğŸ¤” å¸¸è§é—®é¢˜

**Q: è®­ç»ƒéœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ**
A: åœ¨ CPU ä¸Šå¤§çº¦ 10-20 åˆ†é’Ÿï¼ŒGPU ä¸Š 2-5 åˆ†é’Ÿ

**Q: å¦‚ä½•æé«˜æ¨¡å‹æ•ˆæœï¼Ÿ**
A: å¯ä»¥å°è¯•å¢åŠ  EPOCHSã€è°ƒæ•´ LEARNING_RATE æˆ–ä½¿ç”¨æ›´å¤§çš„ HIDDEN_SIZE

**Q: å†…å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ**
A: å‡å° BATCH_SIZEï¼Œæ¯”å¦‚ä» 128 æ”¹ä¸º 64 æˆ– 32

---

## ğŸ‰ æ­å–œï¼

å®Œæˆè¿™ä¸ªé¡¹ç›®åï¼Œä½ å°†æŒæ¡ï¼š

- æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æµç¨‹
- PyTorch æ¡†æ¶çš„ä½¿ç”¨
- RNN ç¥ç»ç½‘ç»œçš„åŸç†
- è‡ªç„¶è¯­è¨€å¤„ç†çš„å…¥é—¨çŸ¥è¯†

**ä¸‹ä¸€æ­¥ï¼š** å°è¯•ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œæˆ–è€…ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒæ¨¡å‹ï¼

## æ¨¡å‹è¯„ä¼°

## ğŸ“Š Loss 3.5 çš„åˆ†æ

### ğŸ¯ ç†è®ºåŸºå‡†å€¼

å¯¹äºæ‚¨çš„é¡¹ç›®é…ç½®ï¼š

- **è¯æ±‡è¡¨å¤§å°**: 9,289 ä¸ªå­—ç¬¦
- **SEQ_LEN = 5**: ç”¨å‰ 5 ä¸ªå­—ç¬¦é¢„æµ‹ç¬¬ 6 ä¸ªå­—ç¬¦
- **æŸå¤±å‡½æ•°**: CrossEntropyLossï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰

**éšæœºçŒœæµ‹çš„ç†è®ºæŸå¤±å€¼**ï¼š

```
ç†è®ºæœ€å¤§æŸå¤± = log(vocab_size) = log(9289) â‰ˆ 9.14
```

### âœ… Loss 3.5 çš„åˆç†æ€§è¯„ä¼°

**1. ç›¸å¯¹è¡¨ç°è‰¯å¥½**

- æ‚¨çš„ loss 3.5 **è¿œä½äº** éšæœºçŒœæµ‹çš„ 9.14
- è¿™è¯´æ˜æ¨¡å‹å·²ç»å­¦åˆ°äº†ä¸€å®šçš„è¯­è¨€æ¨¡å¼
- ç›¸æ¯”éšæœºçŒœæµ‹ï¼Œå‡†ç¡®ç‡æå‡äº†çº¦ **62%**

**2. æŸå¤±å€¼çš„å®é™…å«ä¹‰**

```python
# äº¤å‰ç†µæŸå¤±çš„æ¦‚ç‡è§£é‡Š
import math
perplexity = math.exp(3.5)  # â‰ˆ 30.0
```

- **å›°æƒ‘åº¦ (Perplexity)**: çº¦ 30ï¼Œè¡¨ç¤ºæ¨¡å‹åœ¨æ¯ä¸ªä½ç½®å¹³å‡"å›°æƒ‘"äº 30 ä¸ªå€™é€‰å­—ç¬¦
- è¿™åœ¨ 9289 ä¸ªå­—ç¬¦ä¸­æ˜¯ç›¸å½“ä¸é”™çš„è¡¨ç°

### ğŸš€ ä¼˜åŒ–å»ºè®®

**1. æ¨¡å‹ç»“æ„ä¼˜åŒ–**

```python
# åœ¨ model.py ä¸­å¯ä»¥å°è¯•ï¼š
# 1. ä½¿ç”¨LSTMæ›¿ä»£RNN
self.rnn = nn.LSTM(
    input_size=config.EMBEDDING_DIM,
    hidden_size=config.HIDDEN_SIZE,
    num_layers=2,  # å¢åŠ å±‚æ•°
    dropout=0.2,   # æ·»åŠ dropout
    batch_first=True
)

# 2. å¢åŠ æ¨¡å‹å®¹é‡
HIDDEN_SIZE = 512  # ä»256å¢åŠ åˆ°512
EMBEDDING_DIM = 256  # ä»128å¢åŠ åˆ°256
```

**2. è®­ç»ƒå‚æ•°è°ƒæ•´**

```python
# åœ¨ config.py ä¸­ï¼š
SEQ_LEN = 8        # å¢åŠ åºåˆ—é•¿åº¦ï¼Œæä¾›æ›´å¤šä¸Šä¸‹æ–‡
BATCH_SIZE = 64    # å‡å°æ‰¹æ¬¡å¤§å°ï¼Œæ›´ç¨³å®šçš„æ¢¯åº¦
LEARNING_RATE = 5e-4  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´ç²¾ç»†çš„ä¼˜åŒ–
EPOCHS = 20        # å¢åŠ è®­ç»ƒè½®æ•°
```

**3. æ•°æ®è´¨é‡æå‡**

- ç¡®ä¿è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§
- è€ƒè™‘æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ä¼˜åŒ–
- å¢åŠ è®­ç»ƒæ•°æ®é‡

### ğŸ“ˆ æœŸæœ›çš„æ”¹è¿›ç›®æ ‡

- **çŸ­æœŸç›®æ ‡**: Loss é™åˆ° 2.5-3.0
- **ä¸­æœŸç›®æ ‡**: Loss é™åˆ° 2.0-2.5
- **é•¿æœŸç›®æ ‡**: Loss é™åˆ° 1.5-2.0

### ğŸ” ç›‘æ§å»ºè®®

1. **è§‚å¯ŸæŸå¤±è¶‹åŠ¿**: ç¡®ä¿ loss æŒç»­ä¸‹é™è€Œééœ‡è¡
2. **éªŒè¯é›†è¯„ä¼°**: æ·»åŠ éªŒè¯é›†é¿å…è¿‡æ‹Ÿåˆ
3. **å®é™…æ•ˆæœæµ‹è¯•**: å®šæœŸæµ‹è¯•æ¨¡å‹çš„å®é™…é¢„æµ‹æ•ˆæœ

**æ€»ç»“**: Loss 3.5 åœ¨æ‚¨çš„é…ç½®ä¸‹æ˜¯ä¸€ä¸ª**åˆç†ä¸”æœ‰å¸Œæœ›çš„èµ·ç‚¹**ï¼Œè¯´æ˜æ¨¡å‹æ­£åœ¨å­¦ä¹ ï¼Œä½†ä»æœ‰å¾ˆå¤§çš„ä¼˜åŒ–ç©ºé—´ã€‚é€šè¿‡ä¸Šè¿°å»ºè®®çš„æ¨¡å‹å’Œå‚æ•°è°ƒæ•´ï¼Œåº”è¯¥èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

## æŸ¥çœ‹è®­ç»ƒè®°å½•

```shell
tensorboard --logdir=logs
```
