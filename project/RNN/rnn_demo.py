#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RNN æ—¶é—´åºåˆ—é¢„æµ‹æ¼”ç¤º

æœ¬æ¼”ç¤ºå±•ç¤ºäº†ä½¿ç”¨RNNè¿›è¡Œæ—¶é—´åºåˆ—é¢„æµ‹çš„å®Œæ•´æµç¨‹ï¼š
1. æ•°æ®é›†çš„æ¢³ç†å’Œé¢„å¤„ç†
2. æ¨¡å‹æ„å»ºå’Œè®­ç»ƒ
3. æ¨¡å‹é¢„æµ‹
4. æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024
"""

import numpy as np
import matplotlib.pyplot as plt
import torch
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_processor import DataProcessor
from rnn_model import SimpleRNN, LSTMModel, RNNTrainer

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„RNNæ¼”ç¤ºæµç¨‹
    """
    print("="*60)
    print("RNN æ—¶é—´åºåˆ—é¢„æµ‹æ¼”ç¤º")
    print("="*60)
    
    # ========================================
    # ç¬¬ä¸€æ­¥ï¼šæ•°æ®é›†çš„æ¢³ç†å’Œé¢„å¤„ç†
    # ========================================
    print("\nğŸ” ç¬¬ä¸€æ­¥ï¼šæ•°æ®é›†çš„æ¢³ç†å’Œé¢„å¤„ç†")
    print("-" * 40)
    
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    sequence_length = 20  # æ—¶é—´åºåˆ—é•¿åº¦
    processor = DataProcessor(sequence_length=sequence_length)
    
    # ç”Ÿæˆç¤ºä¾‹æ—¶é—´åºåˆ—æ•°æ®
    print("1.1 ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
    data = processor.generate_sample_data(n_samples=2000)
    
    # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
    info = processor.get_data_info()
    print("\næ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    # æ•°æ®å½’ä¸€åŒ–
    print("\n1.2 æ•°æ®å½’ä¸€åŒ–...")
    scaled_data = processor.normalize_data()
    
    # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†
    print("\n1.3 åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†...")
    X, y = processor.create_sequences()
    
    # åˆ†å‰²è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    print("\n1.4 åˆ†å‰²æ•°æ®é›†...")
    # å…ˆåˆ†å‡ºæµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test = processor.split_data(X, y, test_size=0.2, random_state=42)
    # å†ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›†
    X_train, X_val, y_train, y_val = processor.split_data(X_temp, y_temp, test_size=0.25, random_state=42)
    
    print(f"  è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
    
    # å¯è§†åŒ–åŸå§‹æ•°æ®
    print("\n1.5 æ•°æ®å¯è§†åŒ–...")
    processor.visualize_data(n_points=500)
    
    # ========================================
    # ç¬¬äºŒæ­¥ï¼šæ¨¡å‹æ„å»ºå’Œè®­ç»ƒ
    # ========================================
    print("\nğŸ—ï¸ ç¬¬äºŒæ­¥ï¼šæ¨¡å‹æ„å»ºå’Œè®­ç»ƒ")
    print("-" * 40)
    
    # è®­ç»ƒé…ç½®
    config = {
        'input_size': 1,
        'hidden_size': 64,
        'num_layers': 2,
        'output_size': 1,
        'dropout': 0.2,
        'epochs': 100,
        'batch_size': 32,
        'learning_rate': 0.001,
        'patience': 15
    }
    
    print("\n2.1 æ¨¡å‹é…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # æ¯”è¾ƒä¸åŒæ¨¡å‹
    models_to_compare = {
        'Simple RNN': SimpleRNN(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers'],
            output_size=config['output_size'],
            dropout=config['dropout']
        ),
        'LSTM': LSTMModel(
            input_size=config['input_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers'],
            output_size=config['output_size'],
            dropout=config['dropout']
        )
    }
    
    trained_models = {}
    model_metrics = {}
    
    for model_name, model in models_to_compare.items():
        print(f"\n2.2 è®­ç»ƒ {model_name} æ¨¡å‹...")
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = RNNTrainer(model)
        
        # æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        print(f"  æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train(
            X_train, y_train, X_val, y_val,
            epochs=config['epochs'],
            batch_size=config['batch_size'],
            learning_rate=config['learning_rate'],
            patience=config['patience'],
            save_path=f'best_{model_name.lower().replace(" ", "_")}_model.pth'
        )
        
        # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        trained_models[model_name] = trainer
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        print(f"\n2.3 {model_name} è®­ç»ƒå†å²å¯è§†åŒ–...")
        trainer.plot_training_history()
    
    # ========================================
    # ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹é¢„æµ‹
    # ========================================
    print("\nğŸ”® ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹é¢„æµ‹")
    print("-" * 40)
    
    predictions = {}
    
    for model_name, trainer in trained_models.items():
        print(f"\n3.1 ä½¿ç”¨ {model_name} è¿›è¡Œé¢„æµ‹...")
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
        pred = trainer.predict(X_test)
        predictions[model_name] = pred
        
        print(f"  é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆäº† {len(pred)} ä¸ªé¢„æµ‹å€¼")
    
    # ========================================
    # ç¬¬å››æ­¥ï¼šæ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–
    # ========================================
    print("\nğŸ“Š ç¬¬å››æ­¥ï¼šæ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–")
    print("-" * 40)
    
    # è¯„ä¼°æ‰€æœ‰æ¨¡å‹
    for model_name, trainer in trained_models.items():
        print(f"\n4.1 è¯„ä¼° {model_name} æ¨¡å‹æ€§èƒ½...")
        
        # è¯„ä¼°æ¨¡å‹
        metrics, pred = trainer.evaluate(X_test, y_test)
        model_metrics[model_name] = metrics
        
        # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
        y_test_original = processor.inverse_transform(y_test)
        pred_original = processor.inverse_transform(pred)
        
        # ç»˜åˆ¶é¢„æµ‹ç»“æœ
        print(f"\n4.2 {model_name} é¢„æµ‹ç»“æœå¯è§†åŒ–...")
        trainer.plot_predictions(y_test_original.flatten(), pred_original.flatten(), n_points=200)
    
    # æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    print("\n4.3 æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("-" * 40)
    
    # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨
    metrics_names = ['MSE', 'RMSE', 'MAE', 'MAPE']
    
    print(f"{'æ¨¡å‹':<15} {'MSE':<12} {'RMSE':<12} {'MAE':<12} {'MAPE(%)':<12}")
    print("-" * 65)
    
    for model_name, metrics in model_metrics.items():
        print(f"{model_name:<15} {metrics['MSE']:<12.6f} {metrics['RMSE']:<12.6f} "
              f"{metrics['MAE']:<12.6f} {metrics['MAPE']:<12.2f}")
    
    # ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
    plot_model_comparison(model_metrics)
    
    # ========================================
    # ç¬¬äº”æ­¥ï¼šæœªæ¥é¢„æµ‹æ¼”ç¤º
    # ========================================
    print("\nğŸš€ ç¬¬äº”æ­¥ï¼šæœªæ¥é¢„æµ‹æ¼”ç¤º")
    print("-" * 40)
    
    # é€‰æ‹©æœ€ä½³æ¨¡å‹è¿›è¡Œæœªæ¥é¢„æµ‹
    best_model_name = min(model_metrics.keys(), key=lambda x: model_metrics[x]['RMSE'])
    best_trainer = trained_models[best_model_name]
    
    print(f"\n5.1 ä½¿ç”¨æœ€ä½³æ¨¡å‹ ({best_model_name}) è¿›è¡Œæœªæ¥é¢„æµ‹...")
    
    # è¿›è¡Œå¤šæ­¥é¢„æµ‹
    future_steps = 50
    future_predictions = predict_future(best_trainer, X_test[-1:], future_steps)
    
    # å¯è§†åŒ–æœªæ¥é¢„æµ‹
    plot_future_predictions(processor, y_test, future_predictions, future_steps)
    
    # ========================================
    # æ€»ç»“
    # ========================================
    print("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“")
    print("="*60)
    print(f"âœ… æ•°æ®å¤„ç†: ç”Ÿæˆäº† {len(data)} ä¸ªæ—¶é—´åºåˆ—æ•°æ®ç‚¹")
    print(f"âœ… æ¨¡å‹è®­ç»ƒ: è®­ç»ƒäº† {len(trained_models)} ä¸ªä¸åŒçš„RNNæ¨¡å‹")
    print(f"âœ… æ¨¡å‹è¯„ä¼°: æœ€ä½³æ¨¡å‹æ˜¯ {best_model_name}ï¼ŒRMSE: {model_metrics[best_model_name]['RMSE']:.6f}")
    print(f"âœ… æœªæ¥é¢„æµ‹: é¢„æµ‹äº†æœªæ¥ {future_steps} ä¸ªæ—¶é—´æ­¥")
    print("\nğŸ‰ RNNæ—¶é—´åºåˆ—é¢„æµ‹æ¼”ç¤ºå®Œæˆï¼")
    print("="*60)

def plot_model_comparison(model_metrics):
    """
    ç»˜åˆ¶æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾
    
    Args:
        model_metrics (dict): æ¨¡å‹è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    metrics_names = ['MSE', 'RMSE', 'MAE', 'MAPE']
    model_names = list(model_metrics.keys())
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    for i, metric in enumerate(metrics_names):
        values = [model_metrics[model][metric] for model in model_names]
        
        bars = axes[i].bar(model_names, values, alpha=0.7)
        axes[i].set_title(f'{metric} å¯¹æ¯”')
        axes[i].set_ylabel(metric)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, values):
            height = bar.get_height()
            axes[i].text(bar.get_x() + bar.get_width()/2., height,
                        f'{value:.4f}', ha='center', va='bottom')
        
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, y=1.02)
    plt.show()

def predict_future(trainer, last_sequence, steps):
    """
    è¿›è¡Œå¤šæ­¥æœªæ¥é¢„æµ‹
    
    Args:
        trainer (RNNTrainer): è®­ç»ƒå¥½çš„æ¨¡å‹
        last_sequence (np.array): æœ€åä¸€ä¸ªè¾“å…¥åºåˆ—
        steps (int): é¢„æµ‹æ­¥æ•°
        
    Returns:
        np.array: æœªæ¥é¢„æµ‹å€¼
    """
    predictions = []
    current_sequence = last_sequence.copy()
    
    for _ in range(steps):
        # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
        next_pred = trainer.predict(current_sequence)
        predictions.append(next_pred[0])
        
        # æ›´æ–°åºåˆ—ï¼šç§»é™¤ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæ·»åŠ é¢„æµ‹å€¼
        new_sequence = np.zeros_like(current_sequence)
        new_sequence[0, :-1, :] = current_sequence[0, 1:, :]
        new_sequence[0, -1, 0] = next_pred[0]
        current_sequence = new_sequence
    
    return np.array(predictions)

def plot_future_predictions(processor, y_test, future_predictions, future_steps):
    """
    å¯è§†åŒ–æœªæ¥é¢„æµ‹ç»“æœ
    
    Args:
        processor (DataProcessor): æ•°æ®å¤„ç†å™¨
        y_test (np.array): æµ‹è¯•é›†çœŸå®å€¼
        future_predictions (np.array): æœªæ¥é¢„æµ‹å€¼
        future_steps (int): é¢„æµ‹æ­¥æ•°
    """
    # åå½’ä¸€åŒ–
    y_test_original = processor.inverse_transform(y_test)
    future_original = processor.inverse_transform(future_predictions)
    
    plt.figure(figsize=(15, 8))
    
    # æ˜¾ç¤ºæœ€å100ä¸ªæµ‹è¯•ç‚¹å’Œæœªæ¥é¢„æµ‹
    n_show = min(100, len(y_test_original))
    
    # æµ‹è¯•é›†æ•°æ®
    test_x = range(len(y_test_original) - n_show, len(y_test_original))
    test_y = y_test_original[-n_show:].flatten()
    
    # æœªæ¥é¢„æµ‹æ•°æ®
    future_x = range(len(y_test_original), len(y_test_original) + future_steps)
    future_y = future_original.flatten()
    
    plt.plot(test_x, test_y, label='å†å²æ•°æ®', color='blue', linewidth=2)
    plt.plot(future_x, future_y, label=f'æœªæ¥é¢„æµ‹ ({future_steps}æ­¥)', 
             color='red', linewidth=2, linestyle='--')
    
    # æ·»åŠ åˆ†ç•Œçº¿
    plt.axvline(x=len(y_test_original)-1, color='green', linestyle=':', 
                linewidth=2, label='é¢„æµ‹èµ·ç‚¹')
    
    plt.title('æ—¶é—´åºåˆ—æœªæ¥é¢„æµ‹', fontsize=16)
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('æ•°å€¼')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # æ·»åŠ é¢„æµ‹åŒºåŸŸé˜´å½±
    plt.fill_between(future_x, future_y, alpha=0.3, color='red')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\næœªæ¥ {future_steps} æ­¥é¢„æµ‹ç»Ÿè®¡:")
    print(f"  é¢„æµ‹å‡å€¼: {np.mean(future_original):.4f}")
    print(f"  é¢„æµ‹æ ‡å‡†å·®: {np.std(future_original):.4f}")
    print(f"  é¢„æµ‹èŒƒå›´: [{np.min(future_original):.4f}, {np.max(future_original):.4f}]")

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    np.random.seed(42)
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # è¿è¡Œä¸»ç¨‹åº
    main()