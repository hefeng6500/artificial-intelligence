# Transformer ä¸­è‹±æ–‡ç¿»è¯‘æ¨¡å‹

ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„ä¸­è‹±æ–‡ç¿»è¯‘æ¨¡å‹é¡¹ç›®ï¼Œæ”¯æŒä»åŸå§‹CSVæ•°æ®é›†åˆ°å®Œæ•´è®­ç»ƒå’Œæ¨ç†çš„ç«¯åˆ°ç«¯æµç¨‹ã€‚

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œä½¿ç”¨ Transformer æ¶æ„è¿›è¡Œä¸­æ–‡åˆ°è‹±æ–‡çš„ç¿»è¯‘ã€‚é¡¹ç›®åŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†æµ‹è¯•å’Œæ¼”ç¤ºåŠŸèƒ½ï¼Œç‰¹åˆ«é€‚ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç¥ç»æœºå™¨ç¿»è¯‘æŠ€æœ¯ã€‚

## ä¸»è¦åŠŸèƒ½

- ğŸš€ **å®Œæ•´çš„ Transformer å®ç°**: åŒ…å«ç¼–ç å™¨ã€è§£ç å™¨ã€å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- ğŸ“Š **ç«¯åˆ°ç«¯æ•°æ®å¤„ç†**: ä»åŸå§‹CSVåˆ°è®­ç»ƒå°±ç»ªçš„æ•°æ®é›†
- ğŸ”¤ **åŒè¯­åˆ†è¯å™¨**: æ”¯æŒä¸­è‹±æ–‡æ–‡æœ¬çš„æ™ºèƒ½åˆ†è¯å’Œç¼–ç 
- ğŸ¯ **å¤šç§è§£ç ç­–ç•¥**: è´ªå¿ƒæœç´¢ã€æŸæœç´¢ã€é‡‡æ ·ç”Ÿæˆ
- ğŸ“ˆ **è®­ç»ƒç›‘æ§**: å®æ—¶æŸå¤±è·Ÿè¸ªå’ŒéªŒè¯æŒ‡æ ‡
- ğŸ’¾ **æ¨¡å‹ç®¡ç†**: å®Œæ•´çš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æœºåˆ¶
- ğŸ§ª **æ¨ç†æµ‹è¯•**: ç‹¬ç«‹çš„æ¨ç†æµ‹è¯•è„šæœ¬
- ğŸ¨ **æ¼”ç¤ºåŠŸèƒ½**: äº¤äº’å¼ç¿»è¯‘æ¼”ç¤º

## é¡¹ç›®ç»“æ„

```
transformer-input-method/
â”œâ”€â”€ config/                    # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.py        # æ¨¡å‹æ¶æ„é…ç½®
â”‚   â””â”€â”€ training_config.py     # è®­ç»ƒå‚æ•°é…ç½®
â”œâ”€â”€ data/                      # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ demo/                  # æ¼”ç¤ºæ•°æ®
â”‚   â”œâ”€â”€ models/                # è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â”‚   â””â”€â”€ tokenizer/         # åˆ†è¯å™¨æ–‡ä»¶
â”‚   â”œâ”€â”€ processed/             # å¤„ç†åçš„è®­ç»ƒæ•°æ®
â”‚   â”‚   â”œâ”€â”€ train.json         # è®­ç»ƒé›†
â”‚   â”‚   â”œâ”€â”€ val.json           # éªŒè¯é›†
â”‚   â”‚   â””â”€â”€ test.json          # æµ‹è¯•é›†
â”‚   â”œâ”€â”€ process_data/          # æ•°æ®å¤„ç†è„šæœ¬
â”‚   â”‚   â””â”€â”€ prepare_data.py    # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚   â””â”€â”€ raw/                   # åŸå§‹æ•°æ®é›†
â”‚       â””â”€â”€ damo_mt_testsets_zh2en_spoken_iwslt1617.csv
â”œâ”€â”€ models/                    # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ attention.py           # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”œâ”€â”€ decoder.py             # Transformerè§£ç å™¨
â”‚   â”œâ”€â”€ encoder.py             # Transformerç¼–ç å™¨
â”‚   â”œâ”€â”€ embedding.py           # è¯åµŒå…¥å±‚
â”‚   â”œâ”€â”€ positional.py          # ä½ç½®ç¼–ç 
â”‚   â””â”€â”€ transformer.py         # å®Œæ•´Transformeræ¨¡å‹
â”œâ”€â”€ training/                  # è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ loss.py                # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ optimizer.py           # ä¼˜åŒ–å™¨é…ç½®
â”‚   â””â”€â”€ trainer.py             # è®­ç»ƒå™¨å®ç°
â”œâ”€â”€ utils/                     # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ data_loader.py         # æ•°æ®åŠ è½½å™¨
â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ tokenizer.py           # åŒè¯­åˆ†è¯å™¨
â”‚   â””â”€â”€ visualization.py       # å¯è§†åŒ–å·¥å…·
â”œâ”€â”€ demo.py                    # äº¤äº’å¼æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ train_with_csv.py          # CSVæ•°æ®è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference_test.py          # æ¨ç†æµ‹è¯•è„šæœ¬
â””â”€â”€ README.md                  # é¡¹ç›®æ–‡æ¡£
```

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 1.9+
- CUDA 11.0+ (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
- Windows/Linux/macOS

### ä¾èµ–å®‰è£…

#### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ pip å®‰è£…

```bash
# æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio
pip install transformers tokenizers
pip install numpy pandas matplotlib seaborn
pip install scikit-learn nltk rouge-score
pip install tqdm tensorboard

# å¯é€‰ä¾èµ–ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
pip install plotly kaleido
```

#### æ–¹æ³•äºŒï¼šä½¿ç”¨ requirements.txtï¼ˆå¦‚æœå­˜åœ¨ï¼‰

```bash
pip install -r requirements.txt
```

### å¿«é€Ÿæ¼”ç¤º

è¿è¡Œæ¼”ç¤ºè„šæœ¬æ¥ä½“éªŒå®Œæ•´åŠŸèƒ½ï¼š

```bash
python demo.py
```

æ¼”ç¤ºåŒ…æ‹¬ï¼š
- åˆ†è¯å™¨åŠŸèƒ½å±•ç¤º
- æ¨¡å‹æ¶æ„ä»‹ç»
- è®­ç»ƒè¿‡ç¨‹æ¼”ç¤º
- ç¿»è¯‘åŠŸèƒ½æµ‹è¯•
- æ³¨æ„åŠ›å¯è§†åŒ–
- è¯„ä¼°æŒ‡æ ‡è®¡ç®—

## å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡Œæµç¨‹

```bash
# 1. æ•°æ®é¢„å¤„ç†
cd data/process_data
python prepare_data.py

# 2. æ¨¡å‹è®­ç»ƒ
cd ../..
python train_with_csv.py

# 3. æ¨ç†æµ‹è¯•
python inference_test.py

# 4. äº¤äº’å¼æ¼”ç¤º
python demo.py
```

### é¢„æœŸè¾“å‡º

- **æ•°æ®é¢„å¤„ç†**: ç”Ÿæˆè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†JSONæ–‡ä»¶
- **æ¨¡å‹è®­ç»ƒ**: è¾“å‡ºè®­ç»ƒè¿›åº¦ï¼Œä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
- **æ¨ç†æµ‹è¯•**: æ˜¾ç¤ºç¤ºä¾‹ç¿»è¯‘ç»“æœ
- **æ¼”ç¤º**: æä¾›äº¤äº’å¼ç¿»è¯‘ç•Œé¢

## å®Œæ•´ä½¿ç”¨æŒ‡å—

### 1. æ•°æ®å‡†å¤‡

#### æ­¥éª¤1ï¼šå‡†å¤‡åŸå§‹æ•°æ®

ç¡®ä¿åŸå§‹CSVæ•°æ®é›†ä½äºæ­£ç¡®ä½ç½®ï¼š
```
data/raw/damo_mt_testsets_zh2en_spoken_iwslt1617.csv
```

#### æ­¥éª¤2ï¼šæ•°æ®é¢„å¤„ç†

è¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼Œå°†CSVæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ï¼š

```bash
cd data/process_data
python prepare_data.py
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- `data/processed/train.json` - è®­ç»ƒé›†ï¼ˆ80%ï¼‰
- `data/processed/val.json` - éªŒè¯é›†ï¼ˆ10%ï¼‰
- `data/processed/test.json` - æµ‹è¯•é›†ï¼ˆ10%ï¼‰

### 2. æ¨¡å‹è®­ç»ƒ

#### ä½¿ç”¨CSVæ•°æ®è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
python train_with_csv.py
```

è®­ç»ƒè¿‡ç¨‹å°†ï¼š
- è‡ªåŠ¨åˆ›å»ºåˆ†è¯å™¨
- è®­ç»ƒTransformeræ¨¡å‹
- ä¿å­˜æ¨¡å‹åˆ° `data/models/`
- ä¿å­˜åˆ†è¯å™¨åˆ° `data/models/tokenizer/`
- ç”Ÿæˆè®­ç»ƒæ—¥å¿—

#### è®­ç»ƒè¾“å‡º

è®­ç»ƒå®Œæˆåï¼Œå°†åœ¨ `data/models/` ç›®å½•ä¸‹ç”Ÿæˆï¼š
- `final_model.pt` - è®­ç»ƒå¥½çš„æ¨¡å‹
- `test_model.pth` - æµ‹è¯•æ¨¡å‹æ£€æŸ¥ç‚¹
- `tokenizer/` - åˆ†è¯å™¨æ–‡ä»¶
- `config.json` - æ¨¡å‹é…ç½®

### 3. æ¨¡å‹æ¨ç†æµ‹è¯•

è®­ç»ƒå®Œæˆåï¼Œè¿è¡Œæ¨ç†æµ‹è¯•ï¼š

```bash
python inference_test.py
```

è¿™å°†æµ‹è¯•æ¨¡å‹çš„ç¿»è¯‘èƒ½åŠ›ï¼Œè¾“å‡ºç¤ºä¾‹ç¿»è¯‘ç»“æœã€‚

### 4. äº¤äº’å¼æ¼”ç¤º

è¿è¡Œæ¼”ç¤ºè„šæœ¬ä½“éªŒç¿»è¯‘åŠŸèƒ½ï¼š

```bash
python demo.py
```

æ¼”ç¤ºåŠŸèƒ½åŒ…æ‹¬ï¼š
- äº¤äº’å¼ä¸­è¯‘è‹±ç¿»è¯‘
- æ¨¡å‹æ€§èƒ½æµ‹è¯•
- æ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰

### 5. é…ç½®è¯´æ˜

#### è®­ç»ƒé…ç½® (`config/training_config.py`)

```python
class TrainingConfig:
    # æ¨¡å‹æ¶æ„å‚æ•°
    d_model = 512          # æ¨¡å‹ç»´åº¦
    n_heads = 8            # æ³¨æ„åŠ›å¤´æ•°
    n_layers = 6           # å±‚æ•°
    d_ff = 2048           # å‰é¦ˆç½‘ç»œç»´åº¦
    
    # è®­ç»ƒå‚æ•°
    batch_size = 32        # æ‰¹æ¬¡å¤§å°
    learning_rate = 1e-4   # å­¦ä¹ ç‡
    num_epochs = 50        # è®­ç»ƒè½®æ•°
    
    # æ•°æ®å‚æ•°
    max_seq_length = 128   # æœ€å¤§åºåˆ—é•¿åº¦
    vocab_size = 10000     # è¯æ±‡è¡¨å¤§å°
    
    # è·¯å¾„é…ç½®
    data_dir = "data/processed"
    model_save_dir = "data/models"
```

## è¯¦ç»†ä½¿ç”¨

### æ•°æ®æ ¼å¼æ”¯æŒ

é™¤äº†CSVæ•°æ®å¤–ï¼Œè¿˜æ”¯æŒ JSON å’Œ TXT æ ¼å¼ï¼š

**JSON æ ¼å¼**ï¼š
```json
[
  {"source": "ä½ å¥½ï¼Œä¸–ç•Œï¼", "target": "Hello, world!"},
  {"source": "æˆ‘çˆ±å­¦ä¹ ã€‚", "target": "I love learning."}
]
```

**TXT æ ¼å¼**ï¼š
```
ä½ å¥½ï¼Œä¸–ç•Œï¼\tHello, world!
æˆ‘çˆ±å­¦ä¹ ã€‚\tI love learning.
```

### é«˜çº§è®­ç»ƒé€‰é¡¹

#### ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ

```bash
python train.py --config config/training_config.json
```

#### è‡ªå®šä¹‰è®­ç»ƒå‚æ•°

```bash
python train.py \
    --config config/training_config.json \
    --device cuda \
    --log-level INFO \
    --seed 42
```

#### æ¢å¤è®­ç»ƒ

```bash
python train.py \
    --config config/training_config.json \
    --resume checkpoints/model_epoch_10.pt
```

### 3. æ¨¡å‹æ¨ç†

#### äº¤äº’å¼ç¿»è¯‘

```bash
python inference.py \
    --model checkpoints/final_model.pt \
    --tokenizer checkpoints/tokenizer \
    --mode interactive
```

#### ç¿»è¯‘å•ä¸ªæ–‡æœ¬

```bash
python inference.py \
    --model checkpoints/final_model.pt \
    --tokenizer checkpoints/tokenizer \
    --mode text \
    --input "ä½ å¥½ï¼Œä¸–ç•Œï¼"
```

#### æ‰¹é‡ç¿»è¯‘æ–‡ä»¶

```bash
python inference.py \
    --model checkpoints/final_model.pt \
    --tokenizer checkpoints/tokenizer \
    --mode file \
    --input input.txt \
    --output output.txt
```

#### è¯„ä¼°æ¨¡å‹æ€§èƒ½

```bash
python inference.py \
    --model checkpoints/final_model.pt \
    --tokenizer checkpoints/tokenizer \
    --mode evaluate \
    --test-file test_data.json
```

### 4. é…ç½®å‚æ•°

ä¸»è¦é…ç½®å‚æ•°è¯´æ˜ï¼š

#### æ¨¡å‹å‚æ•°
- `d_model`: æ¨¡å‹ç»´åº¦ (é»˜è®¤: 512)
- `n_heads`: æ³¨æ„åŠ›å¤´æ•° (é»˜è®¤: 8)
- `n_layers`: å±‚æ•° (é»˜è®¤: 6)
- `d_ff`: å‰é¦ˆç½‘ç»œç»´åº¦ (é»˜è®¤: 2048)
- `max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 512)
- `dropout`: Dropout ç‡ (é»˜è®¤: 0.1)

#### è®­ç»ƒå‚æ•°
- `batch_size`: æ‰¹å¤§å° (é»˜è®¤: 32)
- `learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)
- `num_epochs`: è®­ç»ƒè½®æ•° (é»˜è®¤: 100)
- `warmup_steps`: é¢„çƒ­æ­¥æ•° (é»˜è®¤: 4000)
- `max_grad_norm`: æ¢¯åº¦è£å‰ª (é»˜è®¤: 1.0)

#### æ•°æ®å‚æ•°
- `vocab_size`: è¯æ±‡è¡¨å¤§å° (é»˜è®¤: 32000)
- `min_freq`: æœ€å°è¯é¢‘ (é»˜è®¤: 2)
- `src_lang`: æºè¯­è¨€ (é»˜è®¤: "zh")
- `tgt_lang`: ç›®æ ‡è¯­è¨€ (é»˜è®¤: "en")

### 5. è§£ç ç­–ç•¥

#### è´ªå¿ƒæœç´¢
```bash
python inference.py \
    --strategy greedy \
    --max-length 100
```

#### æŸæœç´¢
```bash
python inference.py \
    --strategy beam_search \
    --beam-size 5 \
    --max-length 100 \
    --num-return 3
```

#### é‡‡æ ·ç”Ÿæˆ
```bash
python inference.py \
    --strategy sampling \
    --temperature 0.8 \
    --top-k 50 \
    --top-p 0.9 \
    --num-return 3
```

## è¾“å‡ºæ–‡ä»¶è¯´æ˜

### è®­ç»ƒè¾“å‡º

è®­ç»ƒå®Œæˆåï¼Œé¡¹ç›®å°†ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š

```
data/models/
â”œâ”€â”€ final_model.pt          # æœ€ç»ˆè®­ç»ƒæ¨¡å‹
â”œâ”€â”€ test_model.pth          # æµ‹è¯•æ£€æŸ¥ç‚¹
â”œâ”€â”€ config.json             # æ¨¡å‹é…ç½®æ–‡ä»¶
â”œâ”€â”€ tokenizer/              # åˆ†è¯å™¨ç›®å½•
â”‚   â”œâ”€â”€ zh_tokenizer.json   # ä¸­æ–‡åˆ†è¯å™¨
â”‚   â””â”€â”€ en_tokenizer.json   # è‹±æ–‡åˆ†è¯å™¨
â””â”€â”€ training.log            # è®­ç»ƒæ—¥å¿—
```

### æ•°æ®å¤„ç†è¾“å‡º

```
data/processed/
â”œâ”€â”€ train.json              # è®­ç»ƒé›†ï¼ˆ80%æ•°æ®ï¼‰
â”œâ”€â”€ val.json                # éªŒè¯é›†ï¼ˆ10%æ•°æ®ï¼‰
â””â”€â”€ test.json               # æµ‹è¯•é›†ï¼ˆ10%æ•°æ®ï¼‰
```

## å¸¸è§é—®é¢˜å’Œæ³¨æ„äº‹é¡¹

### å¸¸è§é—®é¢˜

**Q: è®­ç»ƒæ—¶å‡ºç°CUDAå†…å­˜ä¸è¶³é”™è¯¯ï¼Ÿ**
A: å‡å°æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰æˆ–ä½¿ç”¨CPUè®­ç»ƒï¼š
```python
# åœ¨ config/training_config.py ä¸­
batch_size = 16  # æˆ–æ›´å°
```

**Q: æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Ÿ**
A: ç¡®ä¿CSVæ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å«ä¸­è‹±æ–‡å¯¹ç…§æ•°æ®ï¼Œä½¿ç”¨UTF-8ç¼–ç ã€‚

**Q: æ¨¡å‹æ¨ç†ç»“æœä¸ç†æƒ³ï¼Ÿ**
A: å¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®æˆ–è°ƒæ•´æ¨¡å‹å‚æ•°ï¼Œå¢åŠ è®­ç»ƒè½®æ•°ã€‚

**Q: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼Ÿ**
A: ç¡®ä¿å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹æ–‡ä»¶ã€‚

### æ³¨æ„äº‹é¡¹

1. **æ•°æ®è´¨é‡**: ç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡è‰¯å¥½ï¼Œä¸­è‹±æ–‡å¯¹ç…§å‡†ç¡®
2. **è®¡ç®—èµ„æº**: è®­ç»ƒéœ€è¦ä¸€å®šçš„è®¡ç®—èµ„æºï¼Œå»ºè®®ä½¿ç”¨GPUåŠ é€Ÿ
3. **å†…å­˜ä½¿ç”¨**: å¤§è¯æ±‡è¡¨å’Œé•¿åºåˆ—ä¼šæ¶ˆè€—æ›´å¤šå†…å­˜
4. **è®­ç»ƒæ—¶é—´**: å®Œæ•´è®­ç»ƒå¯èƒ½éœ€è¦æ•°å°æ—¶åˆ°æ•°å¤©
5. **æ¨¡å‹è¯„ä¼°**: ä½¿ç”¨å¤šç§æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ€§èƒ½

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

- ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ
- è°ƒæ•´æ‰¹æ¬¡å¤§å°å¹³è¡¡é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœæ”¯æŒï¼‰
- å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹é¿å…è®­ç»ƒä¸­æ–­
- ç›‘æ§éªŒè¯æŸå¤±é¿å…è¿‡æ‹Ÿåˆ

## æ¨¡å‹æ¶æ„

### Transformer ç»“æ„

æœ¬é¡¹ç›®å®ç°äº†æ ‡å‡†çš„ Transformer æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š

1. **ç¼–ç å™¨ (Encoder)**
   - å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
   - ä½ç½®å‰é¦ˆç½‘ç»œ
   - æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
   - ä½ç½®ç¼–ç 

2. **è§£ç å™¨ (Decoder)**
   - æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
   - ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
   - ä½ç½®å‰é¦ˆç½‘ç»œ
   - æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

3. **æ³¨æ„åŠ›æœºåˆ¶**
   - å¤šå¤´æ³¨æ„åŠ›
   - ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
   - ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰

### å…³é”®ç‰¹æ€§

- **ä½ç½®ç¼–ç **ï¼šæ”¯æŒæ­£å¼¦ä½ç½®ç¼–ç å’Œå¯å­¦ä¹ ä½ç½®åµŒå…¥
- **æ³¨æ„åŠ›æ©ç **ï¼šæ”¯æŒå¡«å……æ©ç å’Œå‰ç»æ©ç 
- **å±‚å½’ä¸€åŒ–**ï¼šPre-LN å’Œ Post-LN ä¸¤ç§æ–¹å¼
- **æ¿€æ´»å‡½æ•°**ï¼šReLUã€GELUã€Swish ç­‰å¤šç§é€‰æ‹©

## è¯„ä¼°æŒ‡æ ‡

### æ”¯æŒçš„æŒ‡æ ‡

1. **BLEU**: åŒè¯­è¯„ä¼°æ›¿è¡¥ (Bilingual Evaluation Understudy)
2. **METEOR**: åŸºäºè¯å¹²ã€åŒä¹‰è¯å’Œé‡Šä¹‰çš„è¯„ä¼°
3. **ROUGE**: é¢å‘æ‘˜è¦çš„è¯„ä¼°æŒ‡æ ‡
4. **CER**: å­—ç¬¦é”™è¯¯ç‡
5. **WER**: è¯é”™è¯¯ç‡
6. **å›°æƒ‘åº¦**: æ¨¡å‹çš„å›°æƒ‘åº¦æŒ‡æ ‡

### ä½¿ç”¨ç¤ºä¾‹

```python
from utils.metrics import TranslationMetrics, calculate_bleu

# è®¡ç®— BLEU åˆ†æ•°
bleu = calculate_bleu(["Hello world"], [["Hello world"]])
print(f"BLEU: {bleu:.4f}")

# ä½¿ç”¨å®Œæ•´è¯„ä¼°æŒ‡æ ‡
metrics = TranslationMetrics()
meteor = metrics.calculate_meteor("Hello world", "Hello world")
rouge = metrics.calculate_rouge("Hello world", "Hello world")
```

## å¯è§†åŒ–åŠŸèƒ½

### æ³¨æ„åŠ›å¯è§†åŒ–

```python
from utils.visualization import AttentionVisualizer

visualizer = AttentionVisualizer()

# ç»˜åˆ¶æ³¨æ„åŠ›çƒ­åŠ›å›¾
visualizer.plot_attention_heatmap(
    attention_weights,
    source_tokens,
    target_tokens,
    save_path="attention.png"
)

# ç»˜åˆ¶å¤šå¤´æ³¨æ„åŠ›
visualizer.plot_multihead_attention(
    attention_weights,
    source_tokens,
    target_tokens,
    save_path="multihead_attention.png"
)
```

### è®­ç»ƒæ›²çº¿å¯è§†åŒ–

```python
from utils.visualization import TrainingVisualizer

visualizer = TrainingVisualizer()

# ç»˜åˆ¶æŸå¤±æ›²çº¿
visualizer.plot_loss_curves(
    train_losses,
    val_losses,
    save_path="loss_curves.png"
)

# ç»˜åˆ¶å­¦ä¹ ç‡è°ƒåº¦
visualizer.plot_learning_rate_schedule(
    learning_rates,
    save_path="lr_schedule.png"
)
```

## é«˜çº§åŠŸèƒ½

### æ··åˆç²¾åº¦è®­ç»ƒ

åœ¨é…ç½®ä¸­å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š

```python
config.use_amp = True
config.amp_opt_level = "O1"
```

### æ¢¯åº¦ç´¯ç§¯

```python
config.gradient_accumulation_steps = 4
```

### æ—©åœæœºåˆ¶

```python
config.early_stopping = True
config.patience = 10
config.min_delta = 1e-4
```

### å­¦ä¹ ç‡è°ƒåº¦

æ”¯æŒå¤šç§å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š

- `linear`: çº¿æ€§è¡°å‡
- `cosine`: ä½™å¼¦é€€ç«
- `exponential`: æŒ‡æ•°è¡°å‡
- `step`: é˜¶æ¢¯è¡°å‡
- `transformer`: Transformer åŸå§‹è°ƒåº¦

## æ€§èƒ½ä¼˜åŒ–

### è®­ç»ƒä¼˜åŒ–

1. **æ•°æ®å¹¶è¡Œ**ï¼šæ”¯æŒå¤š GPU è®­ç»ƒ
2. **æ··åˆç²¾åº¦**ï¼šå‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼ŒåŠ é€Ÿè®­ç»ƒ
3. **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ
4. **åŠ¨æ€å¡«å……**ï¼šå‡å°‘è®¡ç®—æµªè´¹

### æ¨ç†ä¼˜åŒ–

1. **æ‰¹é‡æ¨ç†**ï¼šæé«˜ååé‡
2. **ç¼“å­˜æœºåˆ¶**ï¼šåŠ é€Ÿè§£ç è¿‡ç¨‹
3. **æŸæœç´¢ä¼˜åŒ–**ï¼šé«˜æ•ˆçš„æŸæœç´¢å®ç°
4. **é‡åŒ–æ”¯æŒ**ï¼šå‡å°‘æ¨¡å‹å¤§å°

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ˜¾å­˜ä¸è¶³**
   - å‡å° `batch_size`
   - å¯ç”¨æ¢¯åº¦ç´¯ç§¯
   - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

2. **è®­ç»ƒä¸æ”¶æ•›**
   - è°ƒæ•´å­¦ä¹ ç‡
   - å¢åŠ é¢„çƒ­æ­¥æ•°
   - æ£€æŸ¥æ•°æ®è´¨é‡

3. **ç¿»è¯‘è´¨é‡å·®**
   - å¢åŠ è®­ç»ƒæ•°æ®
   - è°ƒæ•´æ¨¡å‹å¤§å°
   - ä¼˜åŒ–è¶…å‚æ•°

### è°ƒè¯•æŠ€å·§

1. **å¯ç”¨è¯¦ç»†æ—¥å¿—**ï¼š
   ```bash
   python train.py --log-level DEBUG
   ```

2. **å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡**ï¼š
   ```python
   # åœ¨æ¨ç†æ—¶å¯ç”¨æ³¨æ„åŠ›è¾“å‡º
   config.return_attention = True
   ```

3. **ç›‘æ§è®­ç»ƒæŒ‡æ ‡**ï¼š
   - ä½¿ç”¨ TensorBoard æˆ– Weights & Biases
   - å®šæœŸè¯„ä¼°éªŒè¯é›†

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æ³¨æ„åŠ›æœºåˆ¶

1. åœ¨ `models/attention.py` ä¸­å®ç°æ–°çš„æ³¨æ„åŠ›ç±»
2. åœ¨ `models/transformer.py` ä¸­é›†æˆ
3. æ›´æ–°é…ç½®é€‰é¡¹

### æ·»åŠ æ–°çš„æŸå¤±å‡½æ•°

1. åœ¨ `training/loss.py` ä¸­å®ç°æ–°çš„æŸå¤±ç±»
2. åœ¨ `create_loss_function` ä¸­æ³¨å†Œ
3. æ›´æ–°é…ç½®æ–‡æ¡£

### æ·»åŠ æ–°çš„è¯„ä¼°æŒ‡æ ‡

1. åœ¨ `utils/metrics.py` ä¸­å®ç°æ–°çš„æŒ‡æ ‡å‡½æ•°
2. åœ¨ `TranslationMetrics` ç±»ä¸­é›†æˆ
3. æ›´æ–°è¯„ä¼°è„šæœ¬

## è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. æ¨é€åˆ°åˆ†æ”¯
5. åˆ›å»º Pull Request

## è®¸å¯è¯

MIT License

## è‡´è°¢

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer åŸå§‹è®ºæ–‡
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - ä¼˜ç§€çš„ Transformer æ•™ç¨‹
- PyTorch å›¢é˜Ÿ - æ·±åº¦å­¦ä¹ æ¡†æ¶

## é¡¹ç›®ç‰¹ç‚¹

### æŠ€æœ¯ç‰¹è‰²

- **å®Œæ•´å®ç°**: ä»é›¶å¼€å§‹å®ç°Transformeræ¶æ„
- **ç«¯åˆ°ç«¯æµç¨‹**: æ•°æ®å¤„ç†â†’è®­ç»ƒâ†’æ¨ç†â†’æ¼”ç¤ºå®Œæ•´é“¾è·¯
- **å®ç”¨æ€§å¼º**: ä¸“æ³¨ä¸­è¯‘è‹±ä»»åŠ¡ï¼Œé€‚åˆå­¦ä¹ å’Œç ”ç©¶
- **ä»£ç æ¸…æ™°**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºç†è§£å’Œæ‰©å±•
- **æ–‡æ¡£å®Œå–„**: è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œæ³¨é‡Š

### å­¦ä¹ ä»·å€¼

- æ·±å…¥ç†è§£Transformeræ¶æ„åŸç†
- æŒæ¡ç¥ç»æœºå™¨ç¿»è¯‘å®Œæ•´æµç¨‹
- å­¦ä¹ PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶ä½¿ç”¨
- äº†è§£è‡ªç„¶è¯­è¨€å¤„ç†é¡¹ç›®å¼€å‘

## è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç å’Œæ”¹è¿›å»ºè®®ï¼

### è´¡çŒ®æ–¹å¼

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

### å¼€å‘è§„èŒƒ

- éµå¾ªPEP 8ä»£ç é£æ ¼
- æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£
- ç¼–å†™å•å…ƒæµ‹è¯•
- ç¡®ä¿ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## è‡´è°¢

- æ„Ÿè°¢ PyTorch å›¢é˜Ÿæä¾›ä¼˜ç§€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- æ„Ÿè°¢ Hugging Face æä¾›çš„ Transformers åº“å‚è€ƒ
- æ„Ÿè°¢å¼€æºç¤¾åŒºçš„è´¡çŒ®å’Œæ”¯æŒ

---

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ Starï¼**