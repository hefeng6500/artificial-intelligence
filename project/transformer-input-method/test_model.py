#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ¨¡å‹è„šæœ¬
ç”¨äºå¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.tokenizer import BilingualTokenizer
from models.transformer import TransformerModel
from config.model_config import ModelConfig

def create_simple_test_data():
    """åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®"""
    test_data = [
        ("ä½ å¥½", "hello"),
        ("è°¢è°¢", "thank you"),
        ("å†è§", "goodbye"),
        ("æ—©ä¸Šå¥½", "good morning"),
        ("æ™šå®‰", "good night")
    ]
    return test_data

def test_model():
    """æµ‹è¯•æ¨¡å‹åŠŸèƒ½"""
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    logger.info("å¼€å§‹æµ‹è¯•æ¨¡å‹...")
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_simple_test_data()
    zh_texts = [item[0] for item in test_data]
    en_texts = [item[1] for item in test_data]
    
    # åˆ›å»ºåˆ†è¯å™¨
    logger.info("åˆ›å»ºåˆ†è¯å™¨...")
    tokenizer = BilingualTokenizer()
    tokenizer.build_vocab(zh_texts, en_texts, min_freq=1, max_vocab_size=1000)
    
    logger.info(f"ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {tokenizer.zh_vocab_size}")
    logger.info(f"è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {tokenizer.en_vocab_size}")
    
    # åˆ›å»ºé…ç½®
    ModelConfig.update_config(
        vocab_size_zh=tokenizer.zh_vocab_size,
        vocab_size_en=tokenizer.en_vocab_size,
        d_model=128,  # å‡å°æ¨¡å‹å¤§å°
        n_heads=4,
        n_layers=2,
        d_ff=256,
        max_seq_len=64,
        dropout=0.1
    )
    config = ModelConfig
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("åˆ›å»ºæ¨¡å‹...")
    model = TransformerModel(
        src_vocab_size=tokenizer.zh_vocab_size,
        tgt_vocab_size=tokenizer.en_vocab_size,
        d_model=config.d_model,
        n_heads=config.n_heads,
        n_layers=config.n_layers,
        d_ff=config.d_ff,
        max_seq_len=config.max_seq_len,
        dropout=config.dropout
    )
    logger.info(f"æ¨¡å‹å‚æ•°æ•°: {model.get_trainable_parameters():,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    logger.info("æµ‹è¯•å‰å‘ä¼ æ’­...")
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    src_input = torch.randint(1, tokenizer.zh_vocab_size, (batch_size, seq_len))
    tgt_input = torch.randint(1, tokenizer.en_vocab_size, (batch_size, seq_len))
    
    model.eval()
    with torch.no_grad():
        output = model(src_input, tgt_input)
        logger.info(f"è¾“å‡ºlogitså½¢çŠ¶: {output['logits'].shape}")
    
    # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
    models_dir = project_root / "data" / "models"
    models_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    # ä¿å­˜æ¨¡å‹
    model_path = models_dir / "test_model.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config.get_config_dict(),
        'zh_vocab_size': tokenizer.zh_vocab_size,
        'en_vocab_size': tokenizer.en_vocab_size
    }, model_path)
    
    # ä¿å­˜åˆ†è¯å™¨
    tokenizer_dir = models_dir / "tokenizer"
    tokenizer_dir.mkdir(exist_ok=True)
    tokenizer.save(
        zh_path=str(tokenizer_dir / "zh_tokenizer.json"),
        en_path=str(tokenizer_dir / "en_tokenizer.json")
    )
    
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    logger.info(f"åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {tokenizer_dir}")
    
    # æµ‹è¯•åŠ è½½æ¨¡å‹
    logger.info("æµ‹è¯•åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(model_path, map_location=device)
    
    # é‡æ–°åˆ›å»ºæ¨¡å‹
    # æ›´æ–°é…ç½®
    for key, value in checkpoint['config'].items():
        if hasattr(ModelConfig, key):
            setattr(ModelConfig, key, value)
    
    loaded_model = TransformerModel(
        src_vocab_size=checkpoint['zh_vocab_size'],
        tgt_vocab_size=checkpoint['en_vocab_size'],
        d_model=ModelConfig.d_model,
        n_heads=ModelConfig.n_heads,
        n_layers=ModelConfig.n_layers,
        d_ff=ModelConfig.d_ff,
        max_seq_len=ModelConfig.max_seq_len,
        dropout=ModelConfig.dropout
    )
    loaded_model.load_state_dict(checkpoint['model_state_dict'])
    
    logger.info("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    
    # æµ‹è¯•æ¨ç†
    logger.info("æµ‹è¯•æ¨ç†...")
    loaded_model.eval()
    with torch.no_grad():
        test_output = loaded_model(src_input, tgt_input)
        logger.info(f"æ¨ç†è¾“å‡ºlogitså½¢çŠ¶: {test_output['logits'].shape}")
    
    logger.info("æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
    return True

if __name__ == "__main__":
    try:
        test_model()
        print("\nâœ… æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        print("ğŸ“ æ¨¡å‹æ–‡ä»¶å·²ä¿å­˜åˆ° data/models/ ç›®å½•")
        print("ğŸ”§ å¯ä»¥ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•å’Œæ¨ç†")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)