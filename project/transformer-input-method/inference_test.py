#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨ç†æµ‹è¯•è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œä¸­è¯‘è‹±ç¿»è¯‘æµ‹è¯•
"""

import os
import sys
import torch
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.tokenizer import BilingualTokenizer
from models.transformer import TransformerModel
from config.model_config import ModelConfig

def load_model_and_tokenizer(model_path, tokenizer_dir):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
    logger = logging.getLogger(__name__)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    logger.info("åŠ è½½æ¨¡å‹...")
    checkpoint = torch.load(model_path, map_location=device)
    
    # æ›´æ–°é…ç½®
    for key, value in checkpoint['config'].items():
        if hasattr(ModelConfig, key):
            setattr(ModelConfig, key, value)
    
    # åˆ›å»ºæ¨¡å‹
    model = TransformerModel(
        src_vocab_size=checkpoint['zh_vocab_size'],
        tgt_vocab_size=checkpoint['en_vocab_size'],
        d_model=ModelConfig.d_model,
        n_heads=ModelConfig.n_heads,
        n_layers=ModelConfig.n_layers,
        d_ff=ModelConfig.d_ff,
        max_seq_len=ModelConfig.max_seq_len,
        dropout=ModelConfig.dropout
    )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    # åŠ è½½åˆ†è¯å™¨
    logger.info("åŠ è½½åˆ†è¯å™¨...")
    tokenizer = BilingualTokenizer()
    tokenizer.load(
        zh_path=str(tokenizer_dir / "zh_tokenizer.json"),
        en_path=str(tokenizer_dir / "en_tokenizer.json")
    )
    
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‚æ•°æ•°é‡: {model.get_trainable_parameters():,}")
    logger.info(f"ä¸­æ–‡è¯æ±‡è¡¨å¤§å°: {tokenizer.zh_vocab_size}")
    logger.info(f"è‹±æ–‡è¯æ±‡è¡¨å¤§å°: {tokenizer.en_vocab_size}")
    
    return model, tokenizer, device

def translate_text(model, tokenizer, device, chinese_text, max_length=50):
    """ç¿»è¯‘ä¸­æ–‡æ–‡æœ¬åˆ°è‹±æ–‡"""
    logger = logging.getLogger(__name__)
    
    # åˆ†è¯å’Œç¼–ç 
    zh_tokens = tokenizer.zh_tokenizer.encode(chinese_text)
    
    # è½¬æ¢ä¸ºå¼ é‡
    src_tokens = torch.tensor([zh_tokens], dtype=torch.long, device=device)
    
    logger.info(f"è¾“å…¥: {chinese_text}")
    logger.info(f"ä¸­æ–‡tokens: {zh_tokens}")
    
    # ç”Ÿæˆç¿»è¯‘
    with torch.no_grad():
        generated_tokens = model.generate(
            src_tokens=src_tokens,
            max_length=max_length,
            beam_size=1,  # ä½¿ç”¨è´ªå¿ƒæœç´¢
            temperature=1.0
        )
    
    # è§£ç ç”Ÿæˆçš„tokens
    generated_tokens = generated_tokens[0].cpu().tolist()
    
    # ç§»é™¤ç‰¹æ®Štokens
    if tokenizer.en_tokenizer.sos_token_id in generated_tokens:
        start_idx = generated_tokens.index(tokenizer.en_tokenizer.sos_token_id) + 1
    else:
        start_idx = 0
    
    if tokenizer.en_tokenizer.eos_token_id in generated_tokens[start_idx:]:
        end_idx = generated_tokens.index(tokenizer.en_tokenizer.eos_token_id, start_idx)
        generated_tokens = generated_tokens[start_idx:end_idx]
    else:
        generated_tokens = generated_tokens[start_idx:]
    
    # è§£ç ä¸ºæ–‡æœ¬
    english_text = tokenizer.en_tokenizer.decode(generated_tokens)
    
    logger.info(f"ç”Ÿæˆçš„tokens: {generated_tokens}")
    logger.info(f"è¾“å‡º: {english_text}")
    
    return english_text

def test_inference():
    """æµ‹è¯•æ¨ç†åŠŸèƒ½"""
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    logger.info("å¼€å§‹æ¨ç†æµ‹è¯•...")
    
    # æ¨¡å‹å’Œåˆ†è¯å™¨è·¯å¾„
    models_dir = project_root / "data" / "models"
    model_path = models_dir / "test_model.pth"
    tokenizer_dir = models_dir / "tokenizer"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not model_path.exists():
        logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not tokenizer_dir.exists():
        logger.error(f"åˆ†è¯å™¨ç›®å½•ä¸å­˜åœ¨: {tokenizer_dir}")
        return False
    
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        model, tokenizer, device = load_model_and_tokenizer(model_path, tokenizer_dir)
        
        # æµ‹è¯•ç¿»è¯‘
        test_sentences = [
            "ä½ å¥½",
            "è°¢è°¢",
            "å†è§",
            "æ—©ä¸Šå¥½",
            "æ™šå®‰"
        ]
        
        logger.info("\nå¼€å§‹ç¿»è¯‘æµ‹è¯•:")
        logger.info("=" * 50)
        
        for chinese_text in test_sentences:
            try:
                english_text = translate_text(model, tokenizer, device, chinese_text)
                logger.info(f"ä¸­æ–‡: {chinese_text} -> è‹±æ–‡: {english_text}")
                logger.info("-" * 30)
            except Exception as e:
                logger.error(f"ç¿»è¯‘å¤±è´¥ '{chinese_text}': {e}")
        
        logger.info("æ¨ç†æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        success = test_inference()
        if success:
            print("\nâœ… æ¨ç†æµ‹è¯•æˆåŠŸï¼")
            print("ğŸ¯ æ¨¡å‹å¯ä»¥æ­£å¸¸è¿›è¡Œä¸­è¯‘è‹±ç¿»è¯‘")
        else:
            print("\nâŒ æ¨ç†æµ‹è¯•å¤±è´¥ï¼")
            sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ¨ç†æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)