{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 架构详解与交互式演示\n",
    "\n",
    "本 Notebook 提供了 Transformer 架构的完整实现和交互式演示，帮助深入理解这一革命性的深度学习架构。\n",
    "\n",
    "## 目录\n",
    "1. [Transformer 架构概述](#1-transformer-架构概述)\n",
    "2. [环境设置和依赖导入](#2-环境设置和依赖导入)\n",
    "3. [位置编码详解](#3-位置编码详解)\n",
    "4. [多头自注意力机制](#4-多头自注意力机制)\n",
    "5. [编码器和解码器](#5-编码器和解码器)\n",
    "6. [完整模型构建](#6-完整模型构建)\n",
    "7. [训练演示](#7-训练演示)\n",
    "8. [注意力可视化](#8-注意力可视化)\n",
    "9. [模型推理](#9-模型推理)\n",
    "10. [总结与扩展](#10-总结与扩展)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer 架构概述\n",
    "\n",
    "Transformer 是由 Vaswani 等人在 2017 年提出的一种基于自注意力机制的神经网络架构。它彻底改变了自然语言处理领域，成为了 BERT、GPT 等模型的基础。\n",
    "\n",
    "### 核心创新点：\n",
    "- **自注意力机制**：允许模型直接关注序列中的任意位置\n",
    "- **并行计算**：相比 RNN，可以并行处理整个序列\n",
    "- **位置编码**：通过数学方式编码位置信息\n",
    "- **多头注意力**：从多个角度捕获不同类型的依赖关系\n",
    "\n",
    "### 架构组成：\n",
    "1. **编码器（Encoder）**：处理输入序列\n",
    "2. **解码器（Decoder）**：生成输出序列\n",
    "3. **注意力机制**：核心计算单元\n",
    "4. **前馈网络**：非线性变换\n",
    "5. **残差连接和层归一化**：稳定训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 环境设置和依赖导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "# 设置中文字体以解决可视化中文显示问题\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 检查 CUDA 是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'使用设备: {device}')\n",
    "\n",
    "# 设置图表样式\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 位置编码详解\n",
    "\n",
    "由于 Transformer 没有循环结构，需要通过位置编码来为模型提供序列中的位置信息。\n",
    "\n",
    "### 位置编码公式：\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "其中：\n",
    "- pos：位置索引\n",
    "- i：维度索引\n",
    "- d_model：模型维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 计算除数项\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 应用正弦和余弦函数\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数位置使用 sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数位置使用 cos\n",
    "        \n",
    "        # 添加批次维度并注册为缓冲区\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 可视化位置编码\n",
    "def visualize_positional_encoding(d_model=512, max_len=100):\n",
    "    pe = PositionalEncoding(d_model, max_len)\n",
    "    \n",
    "    # 获取位置编码矩阵\n",
    "    pos_encoding = pe.pe[:max_len, 0, :].numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # 绘制位置编码热力图\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(pos_encoding.T, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('位置编码热力图')\n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('维度')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # 绘制前几个维度的位置编码曲线\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i in range(0, min(8, d_model), 2):\n",
    "        plt.plot(pos_encoding[:, i], label=f'维度 {i}')\n",
    "        plt.plot(pos_encoding[:, i+1], label=f'维度 {i+1}', linestyle='--')\n",
    "    plt.title('位置编码曲线（前8个维度）')\n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('编码值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制不同频率的正弦波\n",
    "    plt.subplot(2, 2, 3)\n",
    "    positions = np.arange(max_len)\n",
    "    for i in [0, 2, 4, 8, 16]:\n",
    "        freq = 1 / (10000 ** (i / d_model))\n",
    "        plt.plot(positions, np.sin(positions * freq), label=f'频率 {freq:.4f}')\n",
    "    plt.title('不同频率的正弦波')\n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('值')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 绘制位置编码的相似性矩阵\n",
    "    plt.subplot(2, 2, 4)\n",
    "    similarity = np.dot(pos_encoding, pos_encoding.T)\n",
    "    plt.imshow(similarity, cmap='viridis', aspect='auto')\n",
    "    plt.title('位置编码相似性矩阵')\n",
    "    plt.xlabel('位置')\n",
    "    plt.ylabel('位置')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 运行位置编码可视化\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 多头自注意力机制\n",
    "\n",
    "自注意力机制是 Transformer 的核心，它允许模型在处理序列中的每个位置时，关注序列中的所有位置。\n",
    "\n",
    "### 注意力计算公式：\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k)V\n",
    "\n",
    "其中：\n",
    "- Q：查询矩阵（Query）\n",
    "- K：键矩阵（Key）\n",
    "- V：值矩阵（Value）\n",
    "- d_k：键的维度\n",
    "\n",
    "### 多头注意力的优势：\n",
    "- 允许模型同时关注不同类型的信息\n",
    "- 增加模型的表达能力\n",
    "- 提供更丰富的特征表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # 每个头的维度\n",
    "        \n",
    "        # 线性投影层\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # Query 投影\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # Key 投影\n",
    "        self.w_v = nn.Linear(d_model, d_model)  # Value 投影\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # 输出投影\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # 应用掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # 应用注意力权重到值\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. 线性投影并重塑为多头格式\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. 计算注意力\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3. 连接多头输出\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 4. 最终线性投影\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 演示注意力机制\n",
    "def demonstrate_attention():\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 10\n",
    "    batch_size = 1\n",
    "    \n",
    "    # 创建多头注意力模块\n",
    "    attention = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # 创建随机输入\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 计算自注意力\n",
    "    output, weights = attention(x, x, x)\n",
    "    \n",
    "    print(f'输入形状: {x.shape}')\n",
    "    print(f'输出形状: {output.shape}')\n",
    "    print(f'注意力权重形状: {weights.shape}')\n",
    "    \n",
    "    # 可视化注意力权重\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 显示前4个头的注意力权重\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        attn_map = weights[0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Blues', aspect='auto')\n",
    "        plt.title(f'注意力头 {i+1}')\n",
    "        plt.xlabel('键位置')\n",
    "        plt.ylabel('查询位置')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# 运行注意力演示\n",
    "output, weights = demonstrate_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 导入完整的 Transformer 模型\n",
    "\n",
    "现在我们导入之前创建的完整 Transformer 模型，并进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入完整的 Transformer 模型\n",
    "from transformer_model import Transformer\n",
    "\n",
    "# 创建模型实例\n",
    "def create_transformer_model():\n",
    "    model = Transformer(\n",
    "        src_vocab_size=1000,\n",
    "        tgt_vocab_size=1000,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_len=100,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # 计算模型参数数量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f'模型总参数数: {total_params:,}')\n",
    "    print(f'可训练参数数: {trainable_params:,}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "model = create_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型推理演示\n",
    "\n",
    "演示如何使用 Transformer 模型进行推理，并可视化注意力权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_model_inference():\n",
    "    # 创建示例输入\n",
    "    batch_size = 2\n",
    "    src_len = 10\n",
    "    tgt_len = 8\n",
    "    \n",
    "    # 随机生成源序列和目标序列\n",
    "    src = torch.randint(1, 100, (batch_size, src_len))\n",
    "    tgt = torch.randint(1, 100, (batch_size, tgt_len))\n",
    "    \n",
    "    print(f'源序列形状: {src.shape}')\n",
    "    print(f'目标序列形状: {tgt.shape}')\n",
    "    \n",
    "    # 模型推理\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, enc_attn, dec_attn = model(src, tgt)\n",
    "    \n",
    "    print(f'输出形状: {output.shape}')\n",
    "    print(f'编码器注意力层数: {len(enc_attn)}')\n",
    "    print(f'解码器注意力层数: {len(dec_attn)}')\n",
    "    \n",
    "    return output, enc_attn, dec_attn, src, tgt\n",
    "\n",
    "# 运行推理演示\n",
    "output, enc_attn, dec_attn, src, tgt = demonstrate_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 注意力权重可视化\n",
    "\n",
    "可视化编码器和解码器的注意力权重，帮助理解模型的注意力模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(enc_attn, dec_attn, layer_idx=0):\n",
    "    # 可视化编码器注意力\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 编码器注意力 - 显示前4个头\n",
    "    plt.suptitle(f'第 {layer_idx} 层注意力权重可视化', fontsize=16)\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        attn_map = enc_attn[layer_idx][0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Blues', aspect='auto')\n",
    "        plt.title(f'编码器头 {i+1}')\n",
    "        plt.xlabel('键位置')\n",
    "        plt.ylabel('查询位置')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    # 解码器自注意力 - 显示前4个头\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i+5)\n",
    "        attn_map = dec_attn[layer_idx][0][0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Reds', aspect='auto')\n",
    "        plt.title(f'解码器头 {i+1}')\n",
    "        plt.xlabel('键位置')\n",
    "        plt.ylabel('查询位置')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化注意力权重\n",
    "visualize_attention_weights(enc_attn, dec_attn, layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练演示\n",
    "\n",
    "演示如何训练 Transformer 模型，包括损失函数计算和优化器设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_training():\n",
    "    # 设置训练参数\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # 模拟训练数据\n",
    "    batch_size = 4\n",
    "    src_len = 12\n",
    "    tgt_len = 10\n",
    "    vocab_size = 1000\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print('开始训练演示...')\n",
    "    \n",
    "    # 训练几个步骤\n",
    "    for step in range(10):\n",
    "        # 生成随机数据\n",
    "        src = torch.randint(1, vocab_size, (batch_size, src_len))\n",
    "        tgt_input = torch.randint(1, vocab_size, (batch_size, tgt_len))\n",
    "        tgt_output = torch.randint(1, vocab_size, (batch_size, tgt_len))\n",
    "        \n",
    "        # 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(src, tgt_input)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 2 == 0:\n",
    "            print(f'步骤 {step}: 损失 = {loss.item():.4f}')\n",
    "    \n",
    "    # 绘制损失曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, 'b-', linewidth=2)\n",
    "    plt.title('训练损失曲线')\n",
    "    plt.xlabel('训练步骤')\n",
    "    plt.ylabel('损失值')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'训练完成！最终损失: {losses[-1]:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 运行训练演示\n",
    "losses = demonstrate_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 模型分析和可视化\n",
    "\n",
    "分析模型的各种特性，包括参数分布、梯度流等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model():\n",
    "    # 分析模型参数分布\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 收集所有参数\n",
    "    all_params = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            all_params.extend(param.data.flatten().numpy())\n",
    "            layer_names.append(name)\n",
    "    \n",
    "    # 参数分布直方图\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(all_params, bins=50, alpha=0.7)\n",
    "    plt.title('模型参数分布')\n",
    "    plt.xlabel('参数值')\n",
    "    plt.ylabel('频次')\n",
    "    \n",
    "    # 不同层的参数统计\n",
    "    layer_stats = []\n",
    "    layer_labels = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'weight' in name:\n",
    "            layer_stats.append([\n",
    "                param.data.mean().item(),\n",
    "                param.data.std().item(),\n",
    "                param.data.min().item(),\n",
    "                param.data.max().item()\n",
    "            ])\n",
    "            layer_labels.append(name.split('.')[0])\n",
    "    \n",
    "    # 参数统计可视化\n",
    "    if layer_stats:\n",
    "        layer_stats = np.array(layer_stats)\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(layer_stats[:, 0], 'o-', label='均值')\n",
    "        plt.plot(layer_stats[:, 1], 's-', label='标准差')\n",
    "        plt.title('各层参数统计')\n",
    "        plt.xlabel('层索引')\n",
    "        plt.ylabel('值')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # 模型结构信息\n",
    "    plt.subplot(2, 3, 3)\n",
    "    module_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        module_counts[module_type] = module_counts.get(module_type, 0) + 1\n",
    "    \n",
    "    # 过滤掉容器模块\n",
    "    filtered_counts = {k: v for k, v in module_counts.items() \n",
    "                      if k not in ['Transformer', 'ModuleList', 'Sequential']}\n",
    "    \n",
    "    if filtered_counts:\n",
    "        plt.bar(range(len(filtered_counts)), list(filtered_counts.values()))\n",
    "        plt.xticks(range(len(filtered_counts)), list(filtered_counts.keys()), rotation=45)\n",
    "        plt.title('模型组件统计')\n",
    "        plt.ylabel('数量')\n",
    "    \n",
    "    # 注意力头分析\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # 创建示例数据进行注意力分析\n",
    "    with torch.no_grad():\n",
    "        src = torch.randint(1, 100, (1, 10))\n",
    "        tgt = torch.randint(1, 100, (1, 8))\n",
    "        _, enc_attn, _ = model(src, tgt)\n",
    "        \n",
    "        # 计算每个头的注意力熵\n",
    "        entropies = []\n",
    "        for layer_attn in enc_attn:\n",
    "            layer_entropy = []\n",
    "            for head in range(layer_attn.size(1)):\n",
    "                attn_weights = layer_attn[0, head]\n",
    "                entropy = -(attn_weights * torch.log(attn_weights + 1e-9)).sum(dim=-1).mean()\n",
    "                layer_entropy.append(entropy.item())\n",
    "            entropies.append(layer_entropy)\n",
    "        \n",
    "        entropies = np.array(entropies)\n",
    "        plt.imshow(entropies, cmap='viridis', aspect='auto')\n",
    "        plt.title('注意力熵分布')\n",
    "        plt.xlabel('注意力头')\n",
    "        plt.ylabel('层')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    # 模型复杂度分析\n",
    "    plt.subplot(2, 3, 5)\n",
    "    param_sizes = []\n",
    "    param_names = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_sizes.append(param.numel())\n",
    "            param_names.append(name.split('.')[0])\n",
    "    \n",
    "    # 按参数数量排序\n",
    "    sorted_indices = np.argsort(param_sizes)[-10:]  # 显示前10个最大的\n",
    "    top_sizes = [param_sizes[i] for i in sorted_indices]\n",
    "    top_names = [param_names[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.barh(range(len(top_sizes)), top_sizes)\n",
    "    plt.yticks(range(len(top_names)), top_names)\n",
    "    plt.title('参数数量分布（前10）')\n",
    "    plt.xlabel('参数数量')\n",
    "    \n",
    "    # 内存使用分析\n",
    "    plt.subplot(2, 3, 6)\n",
    "    memory_usage = []\n",
    "    layer_types = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            memory = module.weight.numel() * 4  # 假设 float32\n",
    "            memory_usage.append(memory / 1024 / 1024)  # 转换为 MB\n",
    "            layer_types.append(type(module).__name__)\n",
    "    \n",
    "    if memory_usage:\n",
    "        plt.pie(memory_usage[:5], labels=layer_types[:5], autopct='%1.1f%%')\n",
    "        plt.title('内存使用分布 (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印模型摘要\n",
    "    print('\\n=== 模型分析摘要 ===')\n",
    "    print(f'总参数数: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    print(f'可训练参数数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "    print(f'模型大小: {sum(p.numel() * 4 for p in model.parameters()) / 1024 / 1024:.2f} MB')\n",
    "    print(f'编码器层数: {len(model.encoder.layers)}')\n",
    "    print(f'解码器层数: {len(model.decoder.layers)}')\n",
    "    print(f'注意力头数: {model.encoder.layers[0].self_attention.num_heads}')\n",
    "    print(f'模型维度: {model.encoder.layers[0].self_attention.d_model}')\n",
    "\n",
    "# 运行模型分析\n",
    "analyze_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结与扩展\n",
    "\n",
    "### 本教程涵盖的内容：\n",
    "\n",
    "1. **Transformer 架构基础**：理解了自注意力机制、位置编码等核心概念\n",
    "2. **模型实现**：从零开始实现了完整的 Transformer 模型\n",
    "3. **可视化分析**：通过多种图表理解模型的工作原理\n",
    "4. **训练演示**：展示了模型的训练过程\n",
    "5. **模型分析**：深入分析了模型的各种特性\n",
    "\n",
    "### 进一步学习方向：\n",
    "\n",
    "1. **预训练模型**：学习 BERT、GPT 等预训练模型\n",
    "2. **优化技术**：学习学习率调度、梯度裁剪等优化技术\n",
    "3. **应用领域**：探索机器翻译、文本生成、问答系统等应用\n",
    "4. **模型变体**：了解 Vision Transformer、DETR 等变体\n",
    "5. **效率优化**：学习模型压缩、量化、蒸馏等技术\n",
    "\n",
    "### 实践建议：\n",
    "\n",
    "- 尝试修改模型参数，观察对性能的影响\n",
    "- 在真实数据集上训练模型\n",
    "- 实现更复杂的注意力机制\n",
    "- 探索不同的位置编码方法\n",
    "- 研究注意力权重的可解释性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后的总结性演示\n",
    "print('🎉 Transformer 架构学习完成！')\n",
    "print('\\n主要收获：')\n",
    "print('1. 理解了自注意力机制的工作原理')\n",
    "print('2. 掌握了位置编码的数学原理')\n",
    "print('3. 实现了完整的 Transformer 模型')\n",
    "print('4. 学会了可视化注意力权重')\n",
    "print('5. 了解了模型训练和分析方法')\n",
    "print('\\n继续探索 Transformer 的无限可能！🚀')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
