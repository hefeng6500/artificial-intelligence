{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer æ¶æ„è¯¦è§£ä¸äº¤äº’å¼æ¼”ç¤º\n",
    "\n",
    "æœ¬ Notebook æä¾›äº† Transformer æ¶æ„çš„å®Œæ•´å®ç°å’Œäº¤äº’å¼æ¼”ç¤ºï¼Œå¸®åŠ©æ·±å…¥ç†è§£è¿™ä¸€é©å‘½æ€§çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚\n",
    "\n",
    "## ç›®å½•\n",
    "1. [Transformer æ¶æ„æ¦‚è¿°](#1-transformer-æ¶æ„æ¦‚è¿°)\n",
    "2. [ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å¯¼å…¥](#2-ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å¯¼å…¥)\n",
    "3. [ä½ç½®ç¼–ç è¯¦è§£](#3-ä½ç½®ç¼–ç è¯¦è§£)\n",
    "4. [å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶](#4-å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶)\n",
    "5. [ç¼–ç å™¨å’Œè§£ç å™¨](#5-ç¼–ç å™¨å’Œè§£ç å™¨)\n",
    "6. [å®Œæ•´æ¨¡å‹æ„å»º](#6-å®Œæ•´æ¨¡å‹æ„å»º)\n",
    "7. [è®­ç»ƒæ¼”ç¤º](#7-è®­ç»ƒæ¼”ç¤º)\n",
    "8. [æ³¨æ„åŠ›å¯è§†åŒ–](#8-æ³¨æ„åŠ›å¯è§†åŒ–)\n",
    "9. [æ¨¡å‹æ¨ç†](#9-æ¨¡å‹æ¨ç†)\n",
    "10. [æ€»ç»“ä¸æ‰©å±•](#10-æ€»ç»“ä¸æ‰©å±•)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer æ¶æ„æ¦‚è¿°\n",
    "\n",
    "Transformer æ˜¯ç”± Vaswani ç­‰äººåœ¨ 2017 å¹´æå‡ºçš„ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚å®ƒå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæˆä¸ºäº† BERTã€GPT ç­‰æ¨¡å‹çš„åŸºç¡€ã€‚\n",
    "\n",
    "### æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š\n",
    "- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šå…è®¸æ¨¡å‹ç›´æ¥å…³æ³¨åºåˆ—ä¸­çš„ä»»æ„ä½ç½®\n",
    "- **å¹¶è¡Œè®¡ç®—**ï¼šç›¸æ¯” RNNï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—\n",
    "- **ä½ç½®ç¼–ç **ï¼šé€šè¿‡æ•°å­¦æ–¹å¼ç¼–ç ä½ç½®ä¿¡æ¯\n",
    "- **å¤šå¤´æ³¨æ„åŠ›**ï¼šä»å¤šä¸ªè§’åº¦æ•è·ä¸åŒç±»å‹çš„ä¾èµ–å…³ç³»\n",
    "\n",
    "### æ¶æ„ç»„æˆï¼š\n",
    "1. **ç¼–ç å™¨ï¼ˆEncoderï¼‰**ï¼šå¤„ç†è¾“å…¥åºåˆ—\n",
    "2. **è§£ç å™¨ï¼ˆDecoderï¼‰**ï¼šç”Ÿæˆè¾“å‡ºåºåˆ—\n",
    "3. **æ³¨æ„åŠ›æœºåˆ¶**ï¼šæ ¸å¿ƒè®¡ç®—å•å…ƒ\n",
    "4. **å‰é¦ˆç½‘ç»œ**ï¼šéçº¿æ€§å˜æ¢\n",
    "5. **æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–**ï¼šç¨³å®šè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import copy\n",
    "from typing import Optional\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“ä»¥è§£å†³å¯è§†åŒ–ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ä½¿ç”¨è®¾å¤‡: {device}')\n",
    "\n",
    "# è®¾ç½®å›¾è¡¨æ ·å¼\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ä½ç½®ç¼–ç è¯¦è§£\n",
    "\n",
    "ç”±äº Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œéœ€è¦é€šè¿‡ä½ç½®ç¼–ç æ¥ä¸ºæ¨¡å‹æä¾›åºåˆ—ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚\n",
    "\n",
    "### ä½ç½®ç¼–ç å…¬å¼ï¼š\n",
    "- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- posï¼šä½ç½®ç´¢å¼•\n",
    "- iï¼šç»´åº¦ç´¢å¼•\n",
    "- d_modelï¼šæ¨¡å‹ç»´åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # è®¡ç®—é™¤æ•°é¡¹\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®ä½¿ç”¨ sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®ä½¿ç”¨ cos\n",
    "        \n",
    "        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶æ³¨å†Œä¸ºç¼“å†²åŒº\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# å¯è§†åŒ–ä½ç½®ç¼–ç \n",
    "def visualize_positional_encoding(d_model=512, max_len=100):\n",
    "    pe = PositionalEncoding(d_model, max_len)\n",
    "    \n",
    "    # è·å–ä½ç½®ç¼–ç çŸ©é˜µ\n",
    "    pos_encoding = pe.pe[:max_len, 0, :].numpy()\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # ç»˜åˆ¶ä½ç½®ç¼–ç çƒ­åŠ›å›¾\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(pos_encoding.T, cmap='RdYlBu', aspect='auto')\n",
    "    plt.title('ä½ç½®ç¼–ç çƒ­åŠ›å›¾')\n",
    "    plt.xlabel('ä½ç½®')\n",
    "    plt.ylabel('ç»´åº¦')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # ç»˜åˆ¶å‰å‡ ä¸ªç»´åº¦çš„ä½ç½®ç¼–ç æ›²çº¿\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i in range(0, min(8, d_model), 2):\n",
    "        plt.plot(pos_encoding[:, i], label=f'ç»´åº¦ {i}')\n",
    "        plt.plot(pos_encoding[:, i+1], label=f'ç»´åº¦ {i+1}', linestyle='--')\n",
    "    plt.title('ä½ç½®ç¼–ç æ›²çº¿ï¼ˆå‰8ä¸ªç»´åº¦ï¼‰')\n",
    "    plt.xlabel('ä½ç½®')\n",
    "    plt.ylabel('ç¼–ç å€¼')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ç»˜åˆ¶ä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢\n",
    "    plt.subplot(2, 2, 3)\n",
    "    positions = np.arange(max_len)\n",
    "    for i in [0, 2, 4, 8, 16]:\n",
    "        freq = 1 / (10000 ** (i / d_model))\n",
    "        plt.plot(positions, np.sin(positions * freq), label=f'é¢‘ç‡ {freq:.4f}')\n",
    "    plt.title('ä¸åŒé¢‘ç‡çš„æ­£å¼¦æ³¢')\n",
    "    plt.xlabel('ä½ç½®')\n",
    "    plt.ylabel('å€¼')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # ç»˜åˆ¶ä½ç½®ç¼–ç çš„ç›¸ä¼¼æ€§çŸ©é˜µ\n",
    "    plt.subplot(2, 2, 4)\n",
    "    similarity = np.dot(pos_encoding, pos_encoding.T)\n",
    "    plt.imshow(similarity, cmap='viridis', aspect='auto')\n",
    "    plt.title('ä½ç½®ç¼–ç ç›¸ä¼¼æ€§çŸ©é˜µ')\n",
    "    plt.xlabel('ä½ç½®')\n",
    "    plt.ylabel('ä½ç½®')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# è¿è¡Œä½ç½®ç¼–ç å¯è§†åŒ–\n",
    "visualize_positional_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "\n",
    "è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®æ—¶ï¼Œå…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®ã€‚\n",
    "\n",
    "### æ³¨æ„åŠ›è®¡ç®—å…¬å¼ï¼š\n",
    "Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- Qï¼šæŸ¥è¯¢çŸ©é˜µï¼ˆQueryï¼‰\n",
    "- Kï¼šé”®çŸ©é˜µï¼ˆKeyï¼‰\n",
    "- Vï¼šå€¼çŸ©é˜µï¼ˆValueï¼‰\n",
    "- d_kï¼šé”®çš„ç»´åº¦\n",
    "\n",
    "### å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿ï¼š\n",
    "- å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨ä¸åŒç±»å‹çš„ä¿¡æ¯\n",
    "- å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›\n",
    "- æä¾›æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦\n",
    "        \n",
    "        # çº¿æ€§æŠ•å½±å±‚\n",
    "        self.w_q = nn.Linear(d_model, d_model)  # Query æŠ•å½±\n",
    "        self.w_k = nn.Linear(d_model, d_model)  # Key æŠ•å½±\n",
    "        self.w_v = nn.Linear(d_model, d_model)  # Value æŠ•å½±\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # è¾“å‡ºæŠ•å½±\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # åº”ç”¨æ©ç \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # è®¡ç®—æ³¨æ„åŠ›æƒé‡\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. çº¿æ€§æŠ•å½±å¹¶é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. è®¡ç®—æ³¨æ„åŠ›\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3. è¿æ¥å¤šå¤´è¾“å‡º\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 4. æœ€ç»ˆçº¿æ€§æŠ•å½±\n",
    "        output = self.w_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶\n",
    "def demonstrate_attention():\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    seq_len = 10\n",
    "    batch_size = 1\n",
    "    \n",
    "    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—\n",
    "    attention = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # åˆ›å»ºéšæœºè¾“å…¥\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # è®¡ç®—è‡ªæ³¨æ„åŠ›\n",
    "    output, weights = attention(x, x, x)\n",
    "    \n",
    "    print(f'è¾“å…¥å½¢çŠ¶: {x.shape}')\n",
    "    print(f'è¾“å‡ºå½¢çŠ¶: {output.shape}')\n",
    "    print(f'æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {weights.shape}')\n",
    "    \n",
    "    # å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # æ˜¾ç¤ºå‰4ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        attn_map = weights[0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Blues', aspect='auto')\n",
    "        plt.title(f'æ³¨æ„åŠ›å¤´ {i+1}')\n",
    "        plt.xlabel('é”®ä½ç½®')\n",
    "        plt.ylabel('æŸ¥è¯¢ä½ç½®')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "# è¿è¡Œæ³¨æ„åŠ›æ¼”ç¤º\n",
    "output, weights = demonstrate_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å¯¼å…¥å®Œæ•´çš„ Transformer æ¨¡å‹\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å¯¼å…¥ä¹‹å‰åˆ›å»ºçš„å®Œæ•´ Transformer æ¨¡å‹ï¼Œå¹¶è¿›è¡Œæ¼”ç¤ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å®Œæ•´çš„ Transformer æ¨¡å‹\n",
    "from transformer_model import Transformer\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "def create_transformer_model():\n",
    "    model = Transformer(\n",
    "        src_vocab_size=1000,\n",
    "        tgt_vocab_size=1000,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_len=100,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f'æ¨¡å‹æ€»å‚æ•°æ•°: {total_params:,}')\n",
    "    print(f'å¯è®­ç»ƒå‚æ•°æ•°: {trainable_params:,}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "model = create_transformer_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ¨¡å‹æ¨ç†æ¼”ç¤º\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Transformer æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå¹¶å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_model_inference():\n",
    "    # åˆ›å»ºç¤ºä¾‹è¾“å…¥\n",
    "    batch_size = 2\n",
    "    src_len = 10\n",
    "    tgt_len = 8\n",
    "    \n",
    "    # éšæœºç”Ÿæˆæºåºåˆ—å’Œç›®æ ‡åºåˆ—\n",
    "    src = torch.randint(1, 100, (batch_size, src_len))\n",
    "    tgt = torch.randint(1, 100, (batch_size, tgt_len))\n",
    "    \n",
    "    print(f'æºåºåˆ—å½¢çŠ¶: {src.shape}')\n",
    "    print(f'ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}')\n",
    "    \n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output, enc_attn, dec_attn = model(src, tgt)\n",
    "    \n",
    "    print(f'è¾“å‡ºå½¢çŠ¶: {output.shape}')\n",
    "    print(f'ç¼–ç å™¨æ³¨æ„åŠ›å±‚æ•°: {len(enc_attn)}')\n",
    "    print(f'è§£ç å™¨æ³¨æ„åŠ›å±‚æ•°: {len(dec_attn)}')\n",
    "    \n",
    "    return output, enc_attn, dec_attn, src, tgt\n",
    "\n",
    "# è¿è¡Œæ¨ç†æ¼”ç¤º\n",
    "output, enc_attn, dec_attn, src, tgt = demonstrate_model_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–\n",
    "\n",
    "å¯è§†åŒ–ç¼–ç å™¨å’Œè§£ç å™¨çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¸®åŠ©ç†è§£æ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_weights(enc_attn, dec_attn, layer_idx=0):\n",
    "    # å¯è§†åŒ–ç¼–ç å™¨æ³¨æ„åŠ›\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # ç¼–ç å™¨æ³¨æ„åŠ› - æ˜¾ç¤ºå‰4ä¸ªå¤´\n",
    "    plt.suptitle(f'ç¬¬ {layer_idx} å±‚æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–', fontsize=16)\n",
    "    \n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        attn_map = enc_attn[layer_idx][0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Blues', aspect='auto')\n",
    "        plt.title(f'ç¼–ç å™¨å¤´ {i+1}')\n",
    "        plt.xlabel('é”®ä½ç½®')\n",
    "        plt.ylabel('æŸ¥è¯¢ä½ç½®')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    # è§£ç å™¨è‡ªæ³¨æ„åŠ› - æ˜¾ç¤ºå‰4ä¸ªå¤´\n",
    "    for i in range(4):\n",
    "        plt.subplot(2, 4, i+5)\n",
    "        attn_map = dec_attn[layer_idx][0][0, i].detach().numpy()\n",
    "        plt.imshow(attn_map, cmap='Reds', aspect='auto')\n",
    "        plt.title(f'è§£ç å™¨å¤´ {i+1}')\n",
    "        plt.xlabel('é”®ä½ç½®')\n",
    "        plt.ylabel('æŸ¥è¯¢ä½ç½®')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "visualize_attention_weights(enc_attn, dec_attn, layer_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. è®­ç»ƒæ¼”ç¤º\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•è®­ç»ƒ Transformer æ¨¡å‹ï¼ŒåŒ…æ‹¬æŸå¤±å‡½æ•°è®¡ç®—å’Œä¼˜åŒ–å™¨è®¾ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_training():\n",
    "    # è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\n",
    "    batch_size = 4\n",
    "    src_len = 12\n",
    "    tgt_len = 10\n",
    "    vocab_size = 1000\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    print('å¼€å§‹è®­ç»ƒæ¼”ç¤º...')\n",
    "    \n",
    "    # è®­ç»ƒå‡ ä¸ªæ­¥éª¤\n",
    "    for step in range(10):\n",
    "        # ç”Ÿæˆéšæœºæ•°æ®\n",
    "        src = torch.randint(1, vocab_size, (batch_size, src_len))\n",
    "        tgt_input = torch.randint(1, vocab_size, (batch_size, tgt_len))\n",
    "        tgt_output = torch.randint(1, vocab_size, (batch_size, tgt_len))\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(src, tgt_input)\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±\n",
    "        loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
    "        \n",
    "        # åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 2 == 0:\n",
    "            print(f'æ­¥éª¤ {step}: æŸå¤± = {loss.item():.4f}')\n",
    "    \n",
    "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses, 'b-', linewidth=2)\n",
    "    plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')\n",
    "    plt.xlabel('è®­ç»ƒæ­¥éª¤')\n",
    "    plt.ylabel('æŸå¤±å€¼')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# è¿è¡Œè®­ç»ƒæ¼”ç¤º\n",
    "losses = demonstrate_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. æ¨¡å‹åˆ†æå’Œå¯è§†åŒ–\n",
    "\n",
    "åˆ†ææ¨¡å‹çš„å„ç§ç‰¹æ€§ï¼ŒåŒ…æ‹¬å‚æ•°åˆ†å¸ƒã€æ¢¯åº¦æµç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model():\n",
    "    # åˆ†ææ¨¡å‹å‚æ•°åˆ†å¸ƒ\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # æ”¶é›†æ‰€æœ‰å‚æ•°\n",
    "    all_params = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            all_params.extend(param.data.flatten().numpy())\n",
    "            layer_names.append(name)\n",
    "    \n",
    "    # å‚æ•°åˆ†å¸ƒç›´æ–¹å›¾\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.hist(all_params, bins=50, alpha=0.7)\n",
    "    plt.title('æ¨¡å‹å‚æ•°åˆ†å¸ƒ')\n",
    "    plt.xlabel('å‚æ•°å€¼')\n",
    "    plt.ylabel('é¢‘æ¬¡')\n",
    "    \n",
    "    # ä¸åŒå±‚çš„å‚æ•°ç»Ÿè®¡\n",
    "    layer_stats = []\n",
    "    layer_labels = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'weight' in name:\n",
    "            layer_stats.append([\n",
    "                param.data.mean().item(),\n",
    "                param.data.std().item(),\n",
    "                param.data.min().item(),\n",
    "                param.data.max().item()\n",
    "            ])\n",
    "            layer_labels.append(name.split('.')[0])\n",
    "    \n",
    "    # å‚æ•°ç»Ÿè®¡å¯è§†åŒ–\n",
    "    if layer_stats:\n",
    "        layer_stats = np.array(layer_stats)\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.plot(layer_stats[:, 0], 'o-', label='å‡å€¼')\n",
    "        plt.plot(layer_stats[:, 1], 's-', label='æ ‡å‡†å·®')\n",
    "        plt.title('å„å±‚å‚æ•°ç»Ÿè®¡')\n",
    "        plt.xlabel('å±‚ç´¢å¼•')\n",
    "        plt.ylabel('å€¼')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # æ¨¡å‹ç»“æ„ä¿¡æ¯\n",
    "    plt.subplot(2, 3, 3)\n",
    "    module_counts = {}\n",
    "    for name, module in model.named_modules():\n",
    "        module_type = type(module).__name__\n",
    "        module_counts[module_type] = module_counts.get(module_type, 0) + 1\n",
    "    \n",
    "    # è¿‡æ»¤æ‰å®¹å™¨æ¨¡å—\n",
    "    filtered_counts = {k: v for k, v in module_counts.items() \n",
    "                      if k not in ['Transformer', 'ModuleList', 'Sequential']}\n",
    "    \n",
    "    if filtered_counts:\n",
    "        plt.bar(range(len(filtered_counts)), list(filtered_counts.values()))\n",
    "        plt.xticks(range(len(filtered_counts)), list(filtered_counts.keys()), rotation=45)\n",
    "        plt.title('æ¨¡å‹ç»„ä»¶ç»Ÿè®¡')\n",
    "        plt.ylabel('æ•°é‡')\n",
    "    \n",
    "    # æ³¨æ„åŠ›å¤´åˆ†æ\n",
    "    plt.subplot(2, 3, 4)\n",
    "    # åˆ›å»ºç¤ºä¾‹æ•°æ®è¿›è¡Œæ³¨æ„åŠ›åˆ†æ\n",
    "    with torch.no_grad():\n",
    "        src = torch.randint(1, 100, (1, 10))\n",
    "        tgt = torch.randint(1, 100, (1, 8))\n",
    "        _, enc_attn, _ = model(src, tgt)\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›ç†µ\n",
    "        entropies = []\n",
    "        for layer_attn in enc_attn:\n",
    "            layer_entropy = []\n",
    "            for head in range(layer_attn.size(1)):\n",
    "                attn_weights = layer_attn[0, head]\n",
    "                entropy = -(attn_weights * torch.log(attn_weights + 1e-9)).sum(dim=-1).mean()\n",
    "                layer_entropy.append(entropy.item())\n",
    "            entropies.append(layer_entropy)\n",
    "        \n",
    "        entropies = np.array(entropies)\n",
    "        plt.imshow(entropies, cmap='viridis', aspect='auto')\n",
    "        plt.title('æ³¨æ„åŠ›ç†µåˆ†å¸ƒ')\n",
    "        plt.xlabel('æ³¨æ„åŠ›å¤´')\n",
    "        plt.ylabel('å±‚')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    # æ¨¡å‹å¤æ‚åº¦åˆ†æ\n",
    "    plt.subplot(2, 3, 5)\n",
    "    param_sizes = []\n",
    "    param_names = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param_sizes.append(param.numel())\n",
    "            param_names.append(name.split('.')[0])\n",
    "    \n",
    "    # æŒ‰å‚æ•°æ•°é‡æ’åº\n",
    "    sorted_indices = np.argsort(param_sizes)[-10:]  # æ˜¾ç¤ºå‰10ä¸ªæœ€å¤§çš„\n",
    "    top_sizes = [param_sizes[i] for i in sorted_indices]\n",
    "    top_names = [param_names[i] for i in sorted_indices]\n",
    "    \n",
    "    plt.barh(range(len(top_sizes)), top_sizes)\n",
    "    plt.yticks(range(len(top_names)), top_names)\n",
    "    plt.title('å‚æ•°æ•°é‡åˆ†å¸ƒï¼ˆå‰10ï¼‰')\n",
    "    plt.xlabel('å‚æ•°æ•°é‡')\n",
    "    \n",
    "    # å†…å­˜ä½¿ç”¨åˆ†æ\n",
    "    plt.subplot(2, 3, 6)\n",
    "    memory_usage = []\n",
    "    layer_types = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            memory = module.weight.numel() * 4  # å‡è®¾ float32\n",
    "            memory_usage.append(memory / 1024 / 1024)  # è½¬æ¢ä¸º MB\n",
    "            layer_types.append(type(module).__name__)\n",
    "    \n",
    "    if memory_usage:\n",
    "        plt.pie(memory_usage[:5], labels=layer_types[:5], autopct='%1.1f%%')\n",
    "        plt.title('å†…å­˜ä½¿ç”¨åˆ†å¸ƒ (MB)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°æ¨¡å‹æ‘˜è¦\n",
    "    print('\\n=== æ¨¡å‹åˆ†ææ‘˜è¦ ===')\n",
    "    print(f'æ€»å‚æ•°æ•°: {sum(p.numel() for p in model.parameters()):,}')\n",
    "    print(f'å¯è®­ç»ƒå‚æ•°æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "    print(f'æ¨¡å‹å¤§å°: {sum(p.numel() * 4 for p in model.parameters()) / 1024 / 1024:.2f} MB')\n",
    "    print(f'ç¼–ç å™¨å±‚æ•°: {len(model.encoder.layers)}')\n",
    "    print(f'è§£ç å™¨å±‚æ•°: {len(model.decoder.layers)}')\n",
    "    print(f'æ³¨æ„åŠ›å¤´æ•°: {model.encoder.layers[0].self_attention.num_heads}')\n",
    "    print(f'æ¨¡å‹ç»´åº¦: {model.encoder.layers[0].self_attention.d_model}')\n",
    "\n",
    "# è¿è¡Œæ¨¡å‹åˆ†æ\n",
    "analyze_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€»ç»“ä¸æ‰©å±•\n",
    "\n",
    "### æœ¬æ•™ç¨‹æ¶µç›–çš„å†…å®¹ï¼š\n",
    "\n",
    "1. **Transformer æ¶æ„åŸºç¡€**ï¼šç†è§£äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç ç­‰æ ¸å¿ƒæ¦‚å¿µ\n",
    "2. **æ¨¡å‹å®ç°**ï¼šä»é›¶å¼€å§‹å®ç°äº†å®Œæ•´çš„ Transformer æ¨¡å‹\n",
    "3. **å¯è§†åŒ–åˆ†æ**ï¼šé€šè¿‡å¤šç§å›¾è¡¨ç†è§£æ¨¡å‹çš„å·¥ä½œåŸç†\n",
    "4. **è®­ç»ƒæ¼”ç¤º**ï¼šå±•ç¤ºäº†æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹\n",
    "5. **æ¨¡å‹åˆ†æ**ï¼šæ·±å…¥åˆ†æäº†æ¨¡å‹çš„å„ç§ç‰¹æ€§\n",
    "\n",
    "### è¿›ä¸€æ­¥å­¦ä¹ æ–¹å‘ï¼š\n",
    "\n",
    "1. **é¢„è®­ç»ƒæ¨¡å‹**ï¼šå­¦ä¹  BERTã€GPT ç­‰é¢„è®­ç»ƒæ¨¡å‹\n",
    "2. **ä¼˜åŒ–æŠ€æœ¯**ï¼šå­¦ä¹ å­¦ä¹ ç‡è°ƒåº¦ã€æ¢¯åº¦è£å‰ªç­‰ä¼˜åŒ–æŠ€æœ¯\n",
    "3. **åº”ç”¨é¢†åŸŸ**ï¼šæ¢ç´¢æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€é—®ç­”ç³»ç»Ÿç­‰åº”ç”¨\n",
    "4. **æ¨¡å‹å˜ä½“**ï¼šäº†è§£ Vision Transformerã€DETR ç­‰å˜ä½“\n",
    "5. **æ•ˆç‡ä¼˜åŒ–**ï¼šå­¦ä¹ æ¨¡å‹å‹ç¼©ã€é‡åŒ–ã€è’¸é¦ç­‰æŠ€æœ¯\n",
    "\n",
    "### å®è·µå»ºè®®ï¼š\n",
    "\n",
    "- å°è¯•ä¿®æ”¹æ¨¡å‹å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ€§èƒ½çš„å½±å“\n",
    "- åœ¨çœŸå®æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹\n",
    "- å®ç°æ›´å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶\n",
    "- æ¢ç´¢ä¸åŒçš„ä½ç½®ç¼–ç æ–¹æ³•\n",
    "- ç ”ç©¶æ³¨æ„åŠ›æƒé‡çš„å¯è§£é‡Šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€åçš„æ€»ç»“æ€§æ¼”ç¤º\n",
    "print('ğŸ‰ Transformer æ¶æ„å­¦ä¹ å®Œæˆï¼')\n",
    "print('\\nä¸»è¦æ”¶è·ï¼š')\n",
    "print('1. ç†è§£äº†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†')\n",
    "print('2. æŒæ¡äº†ä½ç½®ç¼–ç çš„æ•°å­¦åŸç†')\n",
    "print('3. å®ç°äº†å®Œæ•´çš„ Transformer æ¨¡å‹')\n",
    "print('4. å­¦ä¼šäº†å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡')\n",
    "print('5. äº†è§£äº†æ¨¡å‹è®­ç»ƒå’Œåˆ†ææ–¹æ³•')\n",
    "print('\\nç»§ç»­æ¢ç´¢ Transformer çš„æ— é™å¯èƒ½ï¼ğŸš€')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
