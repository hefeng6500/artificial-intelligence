import torch
import numpy as np
from transformer_model import (
    MultiHeadAttention, 
    PositionwiseFeedForward, 
    PositionalEncoding,
    TransformerBlock,
    TransformerEncoder,
    TransformerDecoder,
    Transformer
)

def test_multi_head_attention():
    """
    æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    """
    print("æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶...")
    
    d_model = 128
    num_heads = 8
    seq_len = 10
    batch_size = 2
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›å±‚
    mha = MultiHeadAttention(d_model, num_heads)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output, attention = mha(x, x, x)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, seq_len, d_model), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    assert attention.shape == (batch_size, num_heads, seq_len, seq_len), f"æ³¨æ„åŠ›å½¢çŠ¶é”™è¯¯: {attention.shape}"
    
    # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡æ˜¯å¦å½’ä¸€åŒ–
    attention_sum = torch.sum(attention, dim=-1)
    assert torch.allclose(attention_sum, torch.ones_like(attention_sum), atol=1e-6), "æ³¨æ„åŠ›æƒé‡æœªæ­£ç¡®å½’ä¸€åŒ–"
    
    print("âœ“ å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•é€šè¿‡")

def test_positional_encoding():
    """
    æµ‹è¯•ä½ç½®ç¼–ç 
    """
    print("æµ‹è¯•ä½ç½®ç¼–ç ...")
    
    d_model = 128
    max_len = 100
    
    # åˆ›å»ºä½ç½®ç¼–ç 
    pe = PositionalEncoding(d_model, max_len)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 50
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åº”ç”¨ä½ç½®ç¼–ç 
    output = pe(x)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == x.shape, f"ä½ç½®ç¼–ç åå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    # æ£€æŸ¥ä½ç½®ç¼–ç æ˜¯å¦è¢«æ­£ç¡®æ·»åŠ 
    assert not torch.equal(output, x), "ä½ç½®ç¼–ç æœªè¢«æ·»åŠ "
    
    print("âœ“ ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡")

def test_feedforward():
    """
    æµ‹è¯•å‰é¦ˆç½‘ç»œ
    """
    print("æµ‹è¯•å‰é¦ˆç½‘ç»œ...")
    
    d_model = 128
    d_ff = 512
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»ºå‰é¦ˆç½‘ç»œ
    ff = PositionwiseFeedForward(d_model, d_ff)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output = ff(x)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == x.shape, f"å‰é¦ˆç½‘ç»œè¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    
    print("âœ“ å‰é¦ˆç½‘ç»œæµ‹è¯•é€šè¿‡")

def test_transformer_block():
    """
    æµ‹è¯• Transformer å—
    """
    print("æµ‹è¯• Transformer å—...")
    
    d_model = 128
    num_heads = 8
    d_ff = 512
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»º Transformer å—
    block = TransformerBlock(d_model, num_heads, d_ff)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output, attention = block(x)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == x.shape, f"Transformer å—è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    assert attention.shape == (batch_size, num_heads, seq_len, seq_len), f"æ³¨æ„åŠ›å½¢çŠ¶é”™è¯¯: {attention.shape}"
    
    print("âœ“ Transformer å—æµ‹è¯•é€šè¿‡")

def test_transformer_encoder():
    """
    æµ‹è¯• Transformer ç¼–ç å™¨
    """
    print("æµ‹è¯• Transformer ç¼–ç å™¨...")
    
    vocab_size = 1000
    d_model = 128
    num_heads = 8
    num_layers = 3
    d_ff = 512
    max_len = 100
    batch_size = 2
    seq_len = 10
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = TransformerEncoder(
        vocab_size, d_model, num_heads, num_layers, d_ff, max_len
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    src = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # å‰å‘ä¼ æ’­
    output, attention_weights = encoder(src)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, seq_len, d_model), f"ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    assert len(attention_weights) == num_layers, f"æ³¨æ„åŠ›æƒé‡å±‚æ•°é”™è¯¯: {len(attention_weights)}"
    
    for i, attn in enumerate(attention_weights):
        assert attn.shape == (batch_size, num_heads, seq_len, seq_len), \
            f"ç¬¬ {i} å±‚æ³¨æ„åŠ›å½¢çŠ¶é”™è¯¯: {attn.shape}"
    
    print("âœ“ Transformer ç¼–ç å™¨æµ‹è¯•é€šè¿‡")

def test_transformer_decoder():
    """
    æµ‹è¯• Transformer è§£ç å™¨
    """
    print("æµ‹è¯• Transformer è§£ç å™¨...")
    
    vocab_size = 1000
    d_model = 128
    num_heads = 8
    num_layers = 3
    d_ff = 512
    max_len = 100
    batch_size = 2
    src_len = 10
    tgt_len = 8
    
    # åˆ›å»ºè§£ç å™¨
    decoder = TransformerDecoder(
        vocab_size, d_model, num_heads, num_layers, d_ff, max_len
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))
    encoder_output = torch.randn(batch_size, src_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output, attention_weights = decoder(tgt, encoder_output)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, tgt_len, vocab_size), f"è§£ç å™¨è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    assert len(attention_weights) == num_layers, f"æ³¨æ„åŠ›æƒé‡å±‚æ•°é”™è¯¯: {len(attention_weights)}"
    
    print("âœ“ Transformer è§£ç å™¨æµ‹è¯•é€šè¿‡")

def test_full_transformer():
    """
    æµ‹è¯•å®Œæ•´çš„ Transformer æ¨¡å‹
    """
    print("æµ‹è¯•å®Œæ•´çš„ Transformer æ¨¡å‹...")
    
    src_vocab_size = 1000
    tgt_vocab_size = 1000
    d_model = 128
    num_heads = 8
    num_encoder_layers = 3
    num_decoder_layers = 3
    d_ff = 512
    max_len = 100
    batch_size = 2
    src_len = 10
    tgt_len = 8
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = Transformer(
        src_vocab_size, tgt_vocab_size, d_model, num_heads,
        num_encoder_layers, num_decoder_layers, d_ff, max_len
    )
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    src = torch.randint(0, src_vocab_size, (batch_size, src_len))
    tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))
    
    # å‰å‘ä¼ æ’­
    output, encoder_attention, decoder_attention = model(src, tgt)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, tgt_len, tgt_vocab_size), f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶é”™è¯¯: {output.shape}"
    assert len(encoder_attention) == num_encoder_layers, f"ç¼–ç å™¨æ³¨æ„åŠ›å±‚æ•°é”™è¯¯: {len(encoder_attention)}"
    assert len(decoder_attention) == num_decoder_layers, f"è§£ç å™¨æ³¨æ„åŠ›å±‚æ•°é”™è¯¯: {len(decoder_attention)}"
    
    print("âœ“ å®Œæ•´ Transformer æ¨¡å‹æµ‹è¯•é€šè¿‡")

def test_model_parameters():
    """
    æµ‹è¯•æ¨¡å‹å‚æ•°æ•°é‡
    """
    print("æµ‹è¯•æ¨¡å‹å‚æ•°æ•°é‡...")
    
    # åˆ›å»ºå°å‹æ¨¡å‹
    model = Transformer(
        src_vocab_size=100,
        tgt_vocab_size=100,
        d_model=64,
        num_heads=4,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_ff=256,
        max_len=50
    )
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    assert total_params > 0, "æ¨¡å‹æ²¡æœ‰å‚æ•°"
    assert trainable_params == total_params, "å­˜åœ¨ä¸å¯è®­ç»ƒçš„å‚æ•°"
    
    print("âœ“ æ¨¡å‹å‚æ•°æµ‹è¯•é€šè¿‡")

def test_gradient_flow():
    """
    æµ‹è¯•æ¢¯åº¦æµåŠ¨
    """
    print("æµ‹è¯•æ¢¯åº¦æµåŠ¨...")
    
    # åˆ›å»ºå°å‹æ¨¡å‹
    model = Transformer(
        src_vocab_size=50,
        tgt_vocab_size=50,
        d_model=32,
        num_heads=2,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_ff=128,
        max_len=20
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    src = torch.randint(0, 50, (1, 5))
    tgt = torch.randint(0, 50, (1, 5))
    target = torch.randint(0, 50, (1, 5))
    
    # å‰å‘ä¼ æ’­
    output, _, _ = model(src, tgt)
    
    # è®¡ç®—æŸå¤±
    criterion = torch.nn.CrossEntropyLoss()
    loss = criterion(output.view(-1, 50), target.view(-1))
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    has_gradient = False
    for name, param in model.named_parameters():
        if param.grad is not None and param.grad.abs().sum() > 0:
            has_gradient = True
            break
    
    assert has_gradient, "æ¨¡å‹å‚æ•°æ²¡æœ‰æ¢¯åº¦"
    
    print("âœ“ æ¢¯åº¦æµåŠ¨æµ‹è¯•é€šè¿‡")

def run_all_tests():
    """
    è¿è¡Œæ‰€æœ‰æµ‹è¯•
    """
    print("=== Transformer æ¨¡å‹æµ‹è¯• ===")
    print()
    
    try:
        test_multi_head_attention()
        test_positional_encoding()
        test_feedforward()
        test_transformer_block()
        test_transformer_encoder()
        test_transformer_decoder()
        test_full_transformer()
        test_model_parameters()
        test_gradient_flow()
        
        print()
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("Transformer æ¨¡å‹å®ç°æ­£ç¡®ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    np.random.seed(42)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    run_all_tests()